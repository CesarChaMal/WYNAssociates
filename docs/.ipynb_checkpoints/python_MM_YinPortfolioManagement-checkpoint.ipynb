{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yin's Capital Product\n",
    "\n",
    "This notebook walks readers through the procedure of installing *YinPortfolioManagement*. Here we introduce the following usages.\n",
    "\n",
    "- Installation\n",
    "- Use RNN3 Regressor\n",
    "\n",
    "All rights reserved to Yiqiao Yin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/yiqiao-yin/YinPortfolioManagement.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of RNN3 Regressor\n",
    "\n",
    "What is LSTM or RNN?\n",
    "\n",
    "## Recurrent Neural Network (a sequential model)\n",
    "\n",
    "Given data $X$ and $Y$, we want to feed information forward into a time stamp. Then we form some belief and we make some initial predictions. We investigate our beliefs by looking at the loss function of the initial guesses and the real value. We update our model according to error we observed. \n",
    "\n",
    "## Architecture: Feed-forward\n",
    "\n",
    "Consider data with time stamp\n",
    "$$X_{\\langle 1 \\rangle} \\rightarrow X_{\\langle 2 \\rangle} \\rightarrow \\dots \\rightarrow X_{\\langle T \\rangle}$$\n",
    "and feed-forward architecture pass information through exactly as the following:\n",
    "$$\n",
    "\\text{Information in:} \\rightarrow\n",
    "\\begin{matrix}\n",
    "Y_{\\langle 1 \\rangle}, \\hat{Y}_{\\langle 1 \\rangle} & Y_{\\langle 2 \\rangle}, \\hat{Y}_{\\langle 2 \\rangle} &       & Y_{\\langle T \\rangle}, \\hat{Y}_{\\langle T \\rangle} \\\\\n",
    "\\uparrow               & \\uparrow               &       & \\uparrow \\\\\n",
    "X_{\\langle 1 \\rangle} \\rightarrow    & X_{\\langle 2 \\rangle} \\rightarrow    & \\dots \\rightarrow & X_{\\langle T \\rangle} \\\\\n",
    "\\uparrow               & \\uparrow               &       & \\uparrow \\\\\n",
    "w_{\\langle 1 \\rangle}, b_{0, \\langle 1 \\rangle}    & w_{\\langle 2 \\rangle}, b_{0, \\langle 2 \\rangle}    &       & w_{\\langle T \\rangle}, b_{0, \\langle T \\rangle} \\\\\n",
    "\\end{matrix}\n",
    "\\rightarrow\n",
    "\\text{Form beliefs about } Y_{\\angle T \\rangle}\n",
    "$$\n",
    "while the educated guesses $\\hat{Y}_{\\langle T \\rangle}$ are our beliefs about real $Y$ at time stamp $T$. \n",
    "\n",
    "## Architecture: Feed-backward\n",
    "\n",
    "Let us clearly define our loss function to make sure we have a proper grip of our mistakes. \n",
    "$$\\mathcal{L} = \\sum_t L(\\hat{y}_{\\langle t \\rangle} - y_t)^2$$\n",
    "and we can compute the gradient \n",
    "$$\\triangledown = \\frac{\\partial \\mathcal{L}}{\\partial a}$$\n",
    "and then with respect with parameters $w$ and $b$\n",
    "$$\\frac{\\partial \\triangledown}{\\partial w}, \\frac{\\partial \\triangledown}{\\partial a}$$\n",
    "and now with perspective of where we make our mistakes according to our parameters we can go backward\n",
    "$$\n",
    "\\text{Information in:} \\leftarrow\n",
    "\\underbrace{\n",
    "\\begin{matrix}\n",
    "Y_{\\langle 1 \\rangle}, \\hat{Y}_{\\langle 1 \\rangle} & Y_{\\langle 2 \\rangle}, \\hat{Y}_{\\langle 2 \\rangle} &       & Y_{\\langle T \\rangle}, \\hat{Y}_{\\langle T \\rangle} \\\\\n",
    "\\uparrow               & \\uparrow               &       & \\uparrow \\\\\n",
    "X_{\\langle 1 \\rangle} \\leftarrow    & X_{\\langle 2 \\rangle} \\leftarrow    & \\dots \\leftarrow & X_{\\langle T \\rangle} \\\\\n",
    "\\uparrow               & \\uparrow               &       & \\uparrow \\\\\n",
    "w'_{\\langle 1 \\rangle}, b'_{0, \\langle 1 \\rangle}    & w'_{\\langle 2 \\rangle}, b'_{0, \\langle 2 \\rangle}    &       & w'_{\\langle T \\rangle}, b'_{0, \\langle T \\rangle} \\\\\n",
    "\\end{matrix}}_{\\text{Update: } w, b \\text{ with } w', b'}\n",
    "\\leftarrow\n",
    "\\text{Total Loss: } \\mathcal{L} (\\hat{y}, y)\n",
    "$$\n",
    "and the *update* action in the above architecture is dependent on your optimizer specified in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from YinCapital_forecast.modules import RNN_Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "\n",
      "            MANUAL: To install this python package, please use the following code.\n",
      "\n",
      "            # In a python notebook:\n",
      "            # !pip install git+https://github.com/yiqiao-yin/YinPortfolioManagement.git\n",
      "            # In a command line:\n",
      "            # pip install git+https://github.com/yiqiao-yin/YinPortfolioManagement.git\n",
      "\n",
      "            # Run\n",
      "            tmp = RNN_Regressor(\n",
      "                    start_date =   '2013-01-01',\n",
      "                    end_date   =   '2019-12-6',\n",
      "                    tickers    =   'AAPL',\n",
      "                    cutoff     =   0.8,\n",
      "                    numOfHiddenLayer = 3,\n",
      "                    l1_units   =   50,\n",
      "                    l2_units   =   50,\n",
      "                    l3_units   =   50,\n",
      "                    l4_units   =   30,\n",
      "                    l5_units   =   10,\n",
      "                    dropOutRate =  0.2,\n",
      "                    optimizer  =   'adam',\n",
      "                    loss       =   'mean_squared_error',\n",
      "                    epochs     =   50,\n",
      "                    batch_size =   64,\n",
      "                    plotGraph  =   True,\n",
      "                    verbose    =   True )\n",
      "                    \n",
      "            # Cite\n",
      "            # All Rights Reserved. Â© Yiqiao Yin\n",
      "            \n",
      "------------------------------------------------------------------------------\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "--------------------------------------------------------------------\n",
      "Shape for data frame in training set:\n",
      "Shape of X: (1832, 100, 1) ; Shape of Y: 1832\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "Shape for data frame in testing set:\n",
      "Shape of X: (114, 100, 1) : Shape of Y: 114\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------\n",
      "Let us investigate the sequential models.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 100, 50)           10400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "--------------------------------------------\n",
      "Time Consumption (in sec): 0.4059159755706787\n",
      "Time Consumption (in min): 0.01\n",
      "Time Consumption (in hr): 0.0 2\n",
      "--------------------------------------------\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "tmp = RNN_Regressor(\n",
    "    start_date =   '2013-01-01',\n",
    "    end_date   =   '2021-07-14',\n",
    "    tickers    =   'AAPL',\n",
    "    cutoff     =   0.9,\n",
    "    numOfHiddenLayer = 2,\n",
    "    l1_units   =   50,\n",
    "    l2_units   =   50,\n",
    "    l3_units   =   50,\n",
    "    dropOutRate = 0.3,\n",
    "    optimizer  =   'adam',\n",
    "    loss       =   'mean_squared_error',\n",
    "    epochs     =   15,\n",
    "    batch_size =   64,\n",
    "    plotGraph  =   True,\n",
    "    verbose    =   True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deeper RNN: RNN5\n",
    "\n",
    "A curious question is the the following: does test set performance get better when we increase the number of hidden layers in a RNN architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = RNN_Regressor(\n",
    "    start_date =   '2013-01-01',\n",
    "    end_date   =   '2021-07-14',\n",
    "    tickers    =   'AAPL',\n",
    "    cutoff     =   0.9,\n",
    "    numOfHiddenLayer = 5,\n",
    "    l1_units   =   50,\n",
    "    l2_units   =   50,\n",
    "    l3_units   =   50,\n",
    "    dropOutRate = 0.3,\n",
    "    optimizer  =   'adam',\n",
    "    loss       =   'mean_squared_error',\n",
    "    epochs     =   15,\n",
    "    batch_size =   64,\n",
    "    plotGraph  =   True,\n",
    "    verbose    =   True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ends here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
