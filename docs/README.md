# Document

This folder stores all the document and notebooks.

# Resources

This is list is provided by [Aman Chadha](https://www.linkedin.com/in/amanc?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAOfvYEBZZvCf4uBUNXyTvIRtEJAo4QBlwU&lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3B17Oz6mmZQCe6DHDpjttkog%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_detail_base-actor_container&lici=XFNUXOlRSeeOorUPkvTqAA%3D%3D) from his LinkedIn [post](https://www.linkedin.com/posts/amanc_artificialintelligence-machinelearning-ml-activity-6977146862604623872-p7BI?utm_source=share&utm_medium=member_desktop). Kudos to him.

ðŸ“š Stanford University

ðŸ”¹ CS221 - Artificial Intelligence: Principles and Techniques by [Percy Liang](https://www.linkedin.com/in/ACoAAAAsk4MBx6p5oT50XyRzLU7Pk3bo9g_0xD8?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D) and [Dorsa Sadigh](https://www.linkedin.com/in/ACoAAAQKjtQBRBLSg-t-nLfVKaj8SQiP3KriVU8?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOca_Ovz1DvdtWuz8BfSWL2)

ðŸ”¹ CS229 - Machine Learning by [Andrew Ng](https://www.linkedin.com/in/ACoAAAqBL5gBXJ8MGhQh-qHYyxAOW-DYlHM3VLg?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/gY8a2yZN)

ðŸ”¹ CS230 - Deep Learning by [Andrew Ng](https://www.linkedin.com/in/ACoAAAqBL5gBXJ8MGhQh-qHYyxAOW-DYlHM3VLg?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/gTk-gKPm)

ðŸ”¹ CS231n - Convolutional Neural Networks for Visual Recognition by [Fei-Fei Li](https://profiles.stanford.edu/fei-fei-li) and [Andrej Karpathy](https://www.linkedin.com/in/ACoAAANHUM4Ba-7gLW9R4il3U9t3O_2BkbkSDvU?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/gGUMZH_G), [2017Spring](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv), [2016Winter](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)

ðŸ”¹ CS224n - Natural Language Processing with Deep Learning by [Christopher Manning](https://www.linkedin.com/in/ACoAAAAVVM0BpHy38YF2LzU5kdIpID9pur6TE0g?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/giWDZGVX), [2021Winter](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ), [2019Winter](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)

ðŸ”¹ CS234 - Reinforcement Learning by [Emma Brunskill](https://www.linkedin.com/in/ACoAAAAsuYEBWz8BN7_aSB2Uu4hvpzvFw90IbyI?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/giWDZGVX)

ðŸ”¹ CS330 - Deep Multi-task and Meta Learning by [Chelsea Finn](https://www.linkedin.com/in/ACoAAAAsuYEBWz8BN7_aSB2Uu4hvpzvFw90IbyI?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/gBVCDcpD)

ðŸ”¹ CS25 - Transformers United: [Course](https://lnkd.in/gEtKgHGC)

ðŸ“š Massachusetts Institute of Technology

ðŸ”¹ 6.S191 - Introduction to Deep Learning by [Alexander Amini](https://www.linkedin.com/in/ACoAABCc8oMB5E-I6JGbhs4cpslnH-B9f7kUvK4?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D) and [Ava Amini (Soleimany)](https://www.linkedin.com/in/ACoAAB-kLP0B7swty0c9Ps1cYOvXzVl4viuDyYg?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3Bbr1%2F0SF5SYqu4NOxKP053Q%3D%3D): [Course](https://lnkd.in/gWMUpMQg)

ðŸ”¹ 6.S094 - Deep Learning by [Lex Fridman](https://www.linkedin.com/in/ACoAAAB0ur4B2dLbFAHKCLO0VHtal_ilBhaBFxM?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3B17Oz6mmZQCe6DHDpjttkog%3D%3D): [Course](https://lnkd.in/gcDgqbH6)

ðŸ”¹ 6.S192 - Deep Learning for Art, Aesthetics, and Creativity by [Ali Jahanian](https://www.linkedin.com/in/ACoAAAkODtwB1d6kNDQLCHOi2FCcr9HoFZIXPHI?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3B17Oz6mmZQCe6DHDpjttkog%3D%3D): [Course](https://lnkd.in/gEyRbEZx)

ðŸ“š Carnegie Mellon University

ðŸ”¹ CS/LTI 11-777 Multimodal Machine Learning by [Louis-Philippe Morency](https://www.linkedin.com/in/ACoAAAA0J4YBkXFZK8lXvW5vH9kwYltQpNvsgBE?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3B17Oz6mmZQCe6DHDpjttkog%3D%3D): [Course](https://lnkd.in/gKFJDbU4)

ðŸ“š University College London

ðŸ”¹ COMP M050 Reinforcement Learning by David Silver: [Course](https://lnkd.in/gEpkWmqh)

- Have courses that you found useful? Drop me a message so I can learn too!
- Check out my watch list with all of the above pointers (and a much larger list of such resources and more): https://aman.ai/watch

# Representation Learning

There are three major types of *Representation Learning*.

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).

## Artificial Neural Network

The architecture of a basic Artificial Neural Network (ANN) is the following
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
\text{output: predictions}
$$

When the notion of neural network is mentioned, we assume the architecture above with any particular sets of parameters (number of layers, number of neurons per layer, ..., i.e. these are all tuning parameters). ANN uses two concepts: *forward propagation* and *backward propagation*. 

- *Forward Propagation*: This operation allows information to pass from the input layer all the way to the output layer. There are linear components as well as non-linear components. Recall the basic linear regression and logistic regression models. We have explanatory variables $X$ and we first construct a linear combination of $\sum_{j=1}^p w_j X_j$ while the running index $j$ indicates the $j$th variable. This is then fed into the activation function (the famous ones are ReLU and sigmoid) to create predictions $\hat{Y}$. 
- *Backward Propagation*: This operation allows us to compare the predictions made by the educated guesses generated from the model with the ground truth, i.e. we compare how far away predictions $\hat{Y}$ is from the truth $Y$. This allows us to figure out the exact loss of a model. This is considered as an optimization problem with the following objective function:
$$\min_w \mathcal{L}(\hat{Y}, Y)$$
and the loss function is a matter of choice by the scientist or user of the algorithm. A list of loss functions can be found [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0). This above objective function states the following. The goal is to minimize the loss generated by prediction $\hat{Y}$ and the ground truth $Y$ while subject to the constraint of parameters (or weights) of the model $w$. In other words, we are allowed to change our weights $w$ until we found ourselves satisfied with the error generated with $\hat{Y}$ and $Y$. The quantity of error margin is a matter of choice and it usually is carried out as a relative term from model to model (i.e. this means we need a benchmark to compare and we do not just rely on one model).
- *Regressor*: This type of ANN learns from $X$ and produces an estimated value of $Y$ while $Y$ is continuous. The common loss function is [MSE](https://towardsdatascience.com/https-medium-com-chayankathuria-regression-why-mean-square-error-a8cad2a1c96f). In a neural network architecture that is designed as a regressor (predict a continuous variable, i.e. like regression model), we output one neuron.
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[.], 
\text{output: predictions}
$$

- *Classifier*: This type of ANN learns from $X$ and produes an estimated probability of $Y$ that is a particular discrete value (aka factor or class). The common loss function is [binary cross-entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a). For a neural network architecture that is designed as a classifier (predict a discrete variable, a class, or a label, i.e. like logistic model), we output a certain number of neurons (number should match the number of levels in the response variable). 
$$
\text{input variables:} \rightarrow
[\vdots] \rightarrow [\vdots]
\rightarrow
[:], 
\text{output: predictions}
$$
The above architecture assumes two-class classification (the output has two dots).
- *Optimizer*: The architecture of an ANN consists of input layer (which is the explanatory variables), hidden layer (if any), the output layer (tailored to the type of problems, regressor or classifier), and a loss function. Once the architecture is setup we can use an optimizer to find the weights that are used in the optimizer. A famous optimizer is called [gradient descent](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c). The key of the gradient descent algorithm focuses on using iterated loops to update the parameters $w$ of the model a step at a time. At each step $s$, we compute the gradient of the loss function, i.e. $\nabla \mathcal{L}(\hat{Y}, Y)$. Next, we update the parameters: $w_s = w_{s+1} - \eta \cdot \nabla \mathcal{L}(\hat{Y}, Y)$. Here are some videos I posted: [Gradient Descent](https://www.youtube.com/watch?v=OtLSnzjT5ns), [Adam](https://www.youtube.com/watch?v=AqzK8LeRThM), [ANN Regressor Classifier Summary](https://www.youtube.com/watch?v=zhBLiMdqOdQ), related python scripts are posted [here](https://www.github.com/yiqiao-yin/YinsPy) 
- *Loss*: A loss function measures the distance between the predictions and the ground truth. The bigger the loss, the more mistakes there are in the model, and the worse the performance will be. The loss function is a choice given by the scientist or the user of the software package. A common loss function is *Mean Square Error*. Suppose we have ground truth $Y$ and the prediction $\hat{Y}$. The loss function *Mean Square Error* or *MSE* is defined as $\frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)^2$ while the running index $i$ indicates the observation. Another famous loss function is *Cross-entropy*. In binary classification problems, we recall the mathematical model of a Bernoulli random variable (i.e. recall the coin toss example). A random variable $X$ is said to have Bernoulli distribution or is a Bernoulli random variable if $\mathbb{P}(X=0) = 1-p$ and $\mathbb{P}(X=1) = p$ while $X = 0$ or $X = 1$. We can write $\mathbb{P}(X) = p^x (1-p)^{1-x}$ to be the mathematical model describing a coin-toss-like profile. Last, we take logarithm on the last step and obtain $x \log(p) + (1-p) \log(1-x)$. In reality, this $p$ is a predicted probability, so we replace it with $\hat{y}$. The $x$ is replaced with $y$ because it is the ground truth. Hence, we arrive with $y\log(\hat{y}) + (1 - y)\log(1 - \hat{y})$. A common list of lost function can be seen [here](https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0).
- Sample code can be found in the following:

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).

## Convolutional Neural Network

Convolutional Neural Network (CNN) is built upon the understanding of a basic neural network. In addition, we make the assumption that we are accepting image data. This assumption implies two key information: (i) local information and its intrinsic value, (ii) geospatial architecture may have important relationship amonst each other. Based on this information and understanding of this type of data sets, we have a strong motivation of using a new tool to develop new features to feed in the neural network architecture.

The convolutional operation relies on combining matrices. Some basic matrix algebra can be found [here](https://towardsdatascience.com/basics-of-linear-algebra-for-data-science-9e93ada24e5c). The convolutional operation can be found [here](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/). The following digram is adopted from the sources above.

The convolutional operation can be illustrated in the following diagram. We have a filter (artificially designed) that intends to capture certain pattern in the picture. We apply this filter starting from the top left corner of the image. Then we roll this filter towards right first until we hit the last column. Once we are finished with one column we move the filter down one row and start from the left of the picture. We keep rolling this filter until we hit the bottom right corner of the image.
<p align="center"><img src="https://github.com/yiqiao-yin/Introduction-to-Machine-Learning-Big-Data-and-Application/blob/main/pics/basic-conv-op.png"></img></p>

The architecture below illustrates a simple Convolutional Neural Network (CNN) architecture and the basic forms of operation.
<p align="center"><img src="https://github.com/yiqiao-yin/Introduction-to-Machine-Learning-Big-Data-and-Application/blob/main/pics/basic-cnn.png"></img></p>

Some additional sources:
- Computer Vision Feature Extraction: [post](https://towardsdatascience.com/computer-vision-feature-extraction-101-on-medical-images-part-1-edge-detection-sharpening-42ab8ef0a7cd)
- Building advanced model using Variational Auto-Encoder (VAE): [python](https://blog.keras.io/building-autoencoders-in-keras.html), [R](https://keras.rstudio.com/articles/examples/variational_autoencoder.html)
- Building advanced model using Generalized Adversarial Network (GAN): [python](https://keras.io/examples/generative/dcgan_overriding_train_step/), [R](https://blogs.rstudio.com/ai/posts/2018-08-26-eager-dcgan/)
- Avatarify: [github](https://github.com/alievk/avatarify-python). Please keep the application within a range of practice that does not harm the society of other people!
- Keras Examples: [python](https://keras.io/examples/), [R](https://tensorflow.rstudio.com/tutorials/)
- General AI Blog: [R AI Blog](https://blogs.rstudio.com/ai/)

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).

## Recurrent Neural Network

LSTM is an tweak version of Recurrent Neural Network which forgets or remembers certain information over a long period of time.

Generalize intuition from above to the following:
- The previous cell state (i.e. the information that was present in the memory after the previous time step).
- The previous hidden state (i.e. this is the same as the output of the previous cell).
- The input at the current time step (i.e. the new information that is being fed in at that moment).

Given data $X$ and $Y$, we want to feed information forward into a time stamp. Then we form some belief and we make some initial predictions. We investigate our beliefs by looking at the loss function of the initial guesses and the real value. We update our model according to error we observed. 

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).

## Architecture: Feed-forward

Consider data with time stamp
$$X_{\langle 1 \rangle} \rightarrow X_{\langle 2 \rangle} \rightarrow \dots \rightarrow X_{\langle T \rangle}$$
and feed-forward architecture pass information through exactly as the following:
$$
\text{Information in:} \rightarrow
\begin{matrix}
Y_{\langle 1 \rangle}, \hat{Y}_{\langle 1 \rangle} & Y_{\langle 2 \rangle}, \hat{Y}_{\langle 2 \rangle} &       & Y_{\langle T \rangle}, \hat{Y}_{\langle T \rangle} \\
\uparrow               & \uparrow               &       & \uparrow \\
X_{\langle 1 \rangle} \rightarrow    & X_{\langle 2 \rangle} \rightarrow    & \dots \rightarrow & X_{\langle T \rangle} \\
\uparrow               & \uparrow               &       & \uparrow \\
w_{\langle 1 \rangle}, b_{0, \langle 1 \rangle}    & w_{\langle 2 \rangle}, b_{0, \langle 2 \rangle}    &       & w_{\langle T \rangle}, b_{0, \langle T \rangle} \\
\end{matrix}
\rightarrow
\text{Form beliefs about } Y_{\langle T \rangle}
$$

while the educated guesses $\hat{Y}_{\langle T \rangle}$ are our beliefs about real $Y$ at time stamp $T$. 

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).

## Architecture: Feed-backward

Let us clearly define our loss function to make sure we have a proper grip of our mistakes. 
$$\mathcal{L} = \sum_t L(\hat{y}_{\langle t \rangle} - y_t)^2$$
and we can compute the gradient 
$$\triangledown = \frac{\partial \mathcal{L}}{\partial a}$$
and then with respect with parameters $w$ and $b$
$$\frac{\partial \triangledown}{\partial w}, \frac{\partial \triangledown}{\partial a}$$
and now with perspective of where we make our mistakes according to our parameters we can go backward

$$
\text{Information in:} \leftarrow
\begin{matrix}
Y_{\langle 1 \rangle}, \hat{Y}_{\langle 1 \rangle} & Y_{\langle 2 \rangle}, \hat{Y}_{\langle 2 \rangle} &       & Y_{\langle T \rangle}, \hat{Y}_{\langle T \rangle} \\
\downarrow               & \downarrow               &       & \downarrow \\
X_{\langle 1 \rangle} \leftarrow    & X_{\langle 2 \rangle} \leftarrow    & \dots \leftarrow & X_{\langle T \rangle} \\
\downarrow               & \downarrow               &       & \downarrow \\
\text{Update: } w_{\langle 1 \rangle}, b_{0, \langle 1 \rangle}    & w_{\langle 2 \rangle}, b_{0, \langle 2 \rangle}    &       & w_{\langle T \rangle}, b_{0, \langle T \rangle} \\
\end{matrix}
\leftarrow
\text{Form beliefs about } Y_{\langle T \rangle}
$$

and the *update* action in the above architecture is dependent on your optimizer specified in the algorithm.

Go back up to the top, please click [here](https://github.com/yiqiao-yin/YinPortfolioManagement/blob/main/docs/README.md).
