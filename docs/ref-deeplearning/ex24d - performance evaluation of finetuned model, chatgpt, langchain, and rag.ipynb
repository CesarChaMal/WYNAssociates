{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "YvtPqYDYXemt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai google-search-results chromadb pypdf sentence_transformers"
      ],
      "metadata": {
        "id": "wxCrkL4vg4m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI as l_OpenAI"
      ],
      "metadata": {
        "id": "CeE8-wVchBLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Keys"
      ],
      "metadata": {
        "id": "UsPQw_KXXgos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SERPAPI_API_KEY = \"xxx\"\n",
        "OPENAI_API_KEY = \"sk-xxx\""
      ],
      "metadata": {
        "id": "JKAm6DMbg70L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "0o21j5ejo6Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "aE0wcqVtWCx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "bp-qjh5Po4QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1\n",
        "\n",
        "Fine-tuning a language model (LLM) like GPT (Generative Pre-trained Transformer) on OpenAI involves customizing the model's behavior by training it further on a specific dataset. This process adjusts the model's parameters so that it becomes more adept at generating responses or completing tasks that are similar to the examples in the dataset used for fine-tuning. The objective is to tailor the model's responses to better align with the domain-specific language, style, or requirements of the dataset."
      ],
      "metadata": {
        "id": "HBINXpAmXiPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High-Level Process\n",
        "\n",
        "The code snippet you've provided is a Python function that interfaces with a fine-tuned model hosted by OpenAI's API. Here's a breakdown of its components:\n",
        "\n",
        "1. **Function Definition**: `def model_finetune(query: str) -> str` defines a function named `model_finetune` that takes a string `query` as input and returns a string. The input `query` is the prompt that you want to send to the fine-tuned model.\n",
        "\n",
        "2. **Completion Request**: Within the function, `openai_client.completions.create()` is called. This function is part of the OpenAI API client library. It requests the API to generate a completion (or response) using the specified model and prompt.\n",
        "\n",
        "    - `model=\"ft:davinci-002:personal::8JEsV0S6\"` specifies the model to use for the completion. Here, `\"ft:davinci-002:personal::8JEsV0S6\"` indicates a specific fine-tuned version of the Davinci model. The `ft` prefix stands for fine-tuned, `davinci-002` is the base model type, and `personal::8JEsV0S6` uniquely identifies your fine-tuned model version. The fine-tuning was performed using 12 CSV files as indicated in your comment, which means your model has been trained to understand and generate responses based on the data in those CSVs.\n",
        "\n",
        "3. **Prompt**: The `prompt` parameter in the function call is set to the `query` variable, which means the text you provide as input to the `model_finetune` function is sent to the model as the prompt for generating a completion.\n",
        "\n",
        "4. **Return Value**: The model's response is accessed via `completion.choices[0].text`. The `choices` list contains possible completions generated by the model, and `[0]` selects the first (and typically only) completion. The `.text` property extracts the textual content of the completion. This text is then returned by the `model_finetune` function.\n",
        "\n",
        "In essence, when you call `model_finetune` with a specific query, it sends this query to your fine-tuned version of the Davinci model on OpenAI's API. The model generates a response based on how it was fine-tuned with your specific dataset (the 12 CSVs), and this response is returned by the function. This allows you to leverage the power of a large language model while ensuring the responses are tailored to the nuances of your specific data and use case."
      ],
      "metadata": {
        "id": "boY9nZjfHHNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is API call to request fine tuned model."
      ],
      "metadata": {
        "id": "jWr4BSYYGW7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_finetune(query: str) -> str:\n",
        "    completion = openai_client.completions.create(\n",
        "        model=\"ft:davinci-002:personal::8JEsV0S6\", # fine tuned model using customized data\n",
        "        prompt=query\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].text"
      ],
      "metadata": {
        "id": "_Xq0lIk5WxUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2\n",
        "\n",
        "When you make an API call to `openai_client.chat.completions.create()` function as illustrated in your Python function `call_chatgpt`, you are essentially interacting with OpenAI's GPT (Generative Pre-trained Transformer) models in a chat-like format. This API is designed to facilitate conversational responses from the model, making it well-suited for applications that require interactive dialogues or conversational AI capabilities. Here's a high-level explanation of the process and components involved in the function:\n"
      ],
      "metadata": {
        "id": "wti4HUIcXmK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### High-Level Process\n",
        "\n",
        "1. **Function Invocation**: The function `call_chatgpt` is called with a `query` (the user's input) and an optional `model` parameter (the specific version of GPT you want to use, defaulting to \"gpt-3.5-turbo\").\n",
        "\n",
        "2. **Conversation Context Preparation**: Before making the API call, the function prepares a conversation context. This context is represented as a list of messages, each with a `role` (either \"system\" or \"user\") and `content`. The \"system\" message sets the stage for the type of assistant the model should emulate (in this case, \"You are a helpful assistant.\"), and the \"user\" message contains the query prefaced by \"Question: \". This structured format helps the model understand the nature of the interaction and respond appropriately.\n",
        "\n",
        "3. **API Call**: With the context prepared, the function then calls `openai_client.chat.completions.create()`, passing the selected `model` and the prepared `messages` as parameters. This API endpoint is specifically designed for generating responses in a conversational context, leveraging the model's ability to understand and continue dialogues.\n",
        "\n",
        "    - `model`: Specifies which version of the GPT model to use for generating the response. Different versions may have different capabilities, performance characteristics, or costs associated with them.\n",
        "    - `messages`: Provides the conversational context to the model, including any system instructions and the user's query.\n",
        "\n",
        "4. **Response Extraction**: The API returns a response object that includes one or more \"choices\", each containing a message. The function extracts the `content` of the message from the first choice, which is the model's response to the user's query within the given context.\n",
        "\n",
        "5. **Return Value**: The extracted content, which is the model-generated response to the query, is returned to the caller.\n",
        "\n",
        "### Summary\n",
        "\n",
        "In summary, calling `openai_client.chat.completions.create()` through the `call_chatgpt` function allows you to engage with an OpenAI GPT model in a conversational manner. By providing a structured conversational context and specifying a model, you can generate responses that are tailored to the input query, simulating a chat with a human-like assistant. This capability is particularly useful for building chatbots, virtual assistants, and other applications where interactive, natural language understanding and generation are required."
      ],
      "metadata": {
        "id": "15XJXZe_Gwjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is API call to ask `chatgpt` directly."
      ],
      "metadata": {
        "id": "Yo4hxWk_GtiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_chatgpt(query: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a response to a query using the specified language model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query that needs to be processed.\n",
        "        model (str, optional): The language model to be used. Defaults to \"gpt-3.5-turbo\".\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare the conversation context with system and user messages.\n",
        "    messages: List[Dict[str, str]] = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {query}.\"},\n",
        "    ]\n",
        "\n",
        "    # Use the OpenAI client to generate a response based on the model and the conversation context.\n",
        "    response: Any = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    # Extract the content of the response from the first choice.\n",
        "    content: str = response.choices[0].message.content\n",
        "\n",
        "    # Return the generated content.\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "wsLFJz05WBJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 3\n"
      ],
      "metadata": {
        "id": "00xMlUUGXsaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `call_langchain` is designed to use LangChain, a framework for building applications with Language Models (LLMs) and integrating various tools for enhanced capabilities, including external data retrieval and processing. This specific function is set up to perform searches using SerpAPI (a service that interfaces with Google Search) and perform mathematical calculations using an integrated LLM, all within a conversational agent context. Here's a breakdown of the high-level operations performed by the function:\n",
        "\n",
        "1. **Initialize the LLM with OpenAI**: The function starts by initializing a language model from OpenAI with a specified `temperature` parameter set to 0, indicating that the responses should be deterministic and not vary between calls. The `OPENAI_API_KEY` is used to authenticate and access OpenAI's API services.\n",
        "\n",
        "2. **Load Required Tools**: It then loads a set of tools using `load_tools`, specifically \"serpapi\" for performing searches via Google Search API, and \"llm-math\" for carrying out mathematical operations. These tools are loaded with their respective API keys (`SERPAPI_API_KEY` for SerpAPI) and the previously initialized language model (`llm`).\n",
        "\n",
        "3. **Initialize Agent**: With the tools and the language model ready, the function initializes an agent using `initialize_agent`. This agent is configured to use the provided tools and language model to interact with prompts. The `AgentType.ZERO_SHOT_REACT_DESCRIPTION` parameter suggests that the agent operates in a zero-shot manner, meaning it attempts to understand and respond to tasks without prior specific training on those tasks. The `verbose=True` parameter likely enables detailed logging or output of the agent's processing for debugging or informational purposes.\n",
        "\n",
        "4. **Run the Agent**: The agent is then run with the user-provided `prompt`. This could be any query or instruction that the user wishes to process, such as a question that requires searching the internet via SerpAPI or a mathematical problem that the \"llm-math\" tool can help solve.\n",
        "\n",
        "5. **Return Output**: Finally, the output from the agent's processing of the prompt is returned. This output could include answers to queries, results of internet searches, solutions to mathematical problems, or any other information the agent is configured to provide based on the prompt and the available tools.\n",
        "\n",
        "In essence, the `call_langchain` function demonstrates a sophisticated integration of AI and external APIs to provide a versatile conversational agent capable of performing specific tasks like web searches and mathematical calculations, leveraging the capabilities of language models for understanding and generating human-like responses."
      ],
      "metadata": {
        "id": "SRX9gN0sHVgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is to use `langchain` to have internet access."
      ],
      "metadata": {
        "id": "I1mS4IlCHVMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_langchain(prompt: str) -> str:\n",
        "    llm = l_OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=SERPAPI_API_KEY)\n",
        "    agent = initialize_agent(\n",
        "        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        "    )\n",
        "    output = agent.run(prompt)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "qNAUcv-Ig4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 4\n"
      ],
      "metadata": {
        "id": "cZxViUAIXxyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you've provided demonstrates an implementation of a Retrieval-Augmented Generation (RAG) algorithm using a Language Model (LLM) for processing and generating responses based on retrieved information from a specialized database. RAG combines the capabilities of information retrieval and generative language models to enhance the accuracy and relevance of responses to queries. Here's a step-by-step breakdown of the process and how the code operates at a high level:\n",
        "\n",
        "### 1. **Setup and Embedding Preparation**\n",
        "- **PDF to Chroma DB Conversion**: Initially, a PDF document is transformed into a format that can be queried effectively. This involves extracting text from the PDF and storing it in a \"Chroma\" database. This database is structured to facilitate efficient retrieval of documents based on semantic similarity.\n",
        "\n",
        "- **Vector DB Creation**: The text data from the Chroma DB is then processed into a vector representation using an embedding function, typically a model like `SentenceTransformerEmbeddingFunction`. This creates a Vector DB where each document or snippet of text from the original PDF is represented as a vector (a list of numbers) in a high-dimensional space. The vector representation allows for measuring semantic similarity between the query and the documents.\n",
        "\n",
        "### 2. **Query Processing**\n",
        "- **Load Chroma Collection**: The `load_chroma` function initializes the Chroma database with the extracted text from the PDF (`pdf_path`) and prepares it for querying. The `embedding_function` is used to ensure that the documents are stored in their vectorized form.\n",
        "\n",
        "- **Document Retrieval**: When the `rag` function is called with a user query, the `chroma_collection.query` function is used to retrieve the most relevant documents based on semantic similarity to the query. It returns a specified number of results (`n_results=10`), including the documents and their embeddings. The most relevant document is then selected (`results['documents'][0]`).\n",
        "\n",
        "### 3. **Augmentation and Response Generation**\n",
        "- **Query Augmentation**: The selected document is then used to augment the original query, framing a new prompt that includes both the user's query and the context provided by the retrieved document. This augmented query is more informative and allows the language model to generate a response that is not only based on the query but also grounded in the specific content of the relevant document.\n",
        "\n",
        "- **Response Generation**: The augmented query is passed to a chat model (e.g., GPT-3.5-turbo via `call_chatgpt` function), which generates a response. This step leverages the generative capabilities of the LLM, using both the original query and the context from the retrieved document to provide a detailed and contextually relevant answer.\n",
        "\n",
        "### 4. **Output**\n",
        "- The function returns the generated response, which ideally combines the generative power of the LLM with the specific, relevant knowledge extracted from the Chroma database.\n",
        "\n",
        "### Summary\n",
        "This RAG implementation enhances the capability of a language model to provide accurate and contextually relevant answers by augmenting its responses with information retrieved from a specialized database. It demonstrates a powerful approach to integrating knowledge retrieval with generative AI, making it particularly useful for applications requiring detailed, accurate, and context-aware responses."
      ],
      "metadata": {
        "id": "7FtjDdlDH8vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector DB Query Search (from scratch)"
      ],
      "metadata": {
        "id": "coDVmTJ3H4P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://raw.githubusercontent.com/yiqiao-yin/WYNAssociates/main/figs/vector%20db%20from%20scratch.png)\n",
        "\n",
        "The above walks through what does the **similarity search** do when we send a `query` into a vector database."
      ],
      "metadata": {
        "id": "SXiFcyaJGGkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/path/to/file/file_name.pdf\""
      ],
      "metadata": {
        "id": "NEIM7It7L_R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "hgQMSlXbbMc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/AI Research/Students/xxx/lectures/2024\")"
      ],
      "metadata": {
        "id": "8TdvnQppNEXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "from helper_utils import load_chroma, word_wrap, project_embeddings"
      ],
      "metadata": {
        "id": "FirZEfIRMy2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path.split('/')[-1].split('.')[0]"
      ],
      "metadata": {
        "id": "pnG0jh-4NxJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embedding_function = SentenceTransformerEmbeddingFunction()\n",
        "\n",
        "nom = pdf_path.split('/')[-1].split('.')[0]\n",
        "chroma_collection = load_chroma(filename=pdf_path, collection_name=f'{nom}', embedding_function=embedding_function)\n",
        "chroma_collection.count()"
      ],
      "metadata": {
        "id": "aBQBt4a1Mt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query: str) -> str:\n",
        "    results = chroma_collection.query(query_texts=query, n_results=10, include=['documents', 'embeddings'])\n",
        "    retrieved_doc = results['documents'][0]\n",
        "\n",
        "    updated_query = f\"\"\"\n",
        "        Answer the question: {query}\n",
        "        Based on the document provided: {retrieved_doc}\n",
        "    \"\"\"\n",
        "    response = call_chatgpt(updated_query)\n",
        "    return response"
      ],
      "metadata": {
        "id": "AYCAhJAlXz-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data\n",
        "\n",
        "This assumes we get `.csv` per topic."
      ],
      "metadata": {
        "id": "kT07CJkMa5zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nom)"
      ],
      "metadata": {
        "id": "5DGTaEmjyMvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_of_csv = f\"file/path/{nom}.csv\""
      ],
      "metadata": {
        "id": "MMDtBUyxa8lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BFkAtcFEcn56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_data = pd.read_csv(path_of_csv)"
      ],
      "metadata": {
        "id": "iie3oLVbcqVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_data.head(2)"
      ],
      "metadata": {
        "id": "1Gecefmfcsdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "HAIFDyb2Xw2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = current_data.questions[0]\n",
        "true_ans = current_data.answers[0]"
      ],
      "metadata": {
        "id": "wWlMK-zuWiFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_finetune = model_finetune(query)\n",
        "ans_finetune"
      ],
      "metadata": {
        "id": "-JPYH-9dXQau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_langchain = call_langchain(query)"
      ],
      "metadata": {
        "id": "rwj9p39_hOmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_langchain"
      ],
      "metadata": {
        "id": "xVWuvRnvWfLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_chatgpt = call_chatgpt(query)\n",
        "ans_chatgpt"
      ],
      "metadata": {
        "id": "KIo15qXahPZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_rag = rag(query)\n",
        "ans_rag"
      ],
      "metadata": {
        "id": "iZlOpcc6Wgn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure it"
      ],
      "metadata": {
        "id": "29r_qsMoX_8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "JCKoflCFYZ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return openai_client.embeddings.create(input = [text], model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "RgJ5PBXYW5kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sts_openai_score(sentence1: str, sentence2: str) -> float:\n",
        "    # Compute sentence embeddings\n",
        "    embedding1 = get_embedding(sentence1)  # Flatten the embedding array\n",
        "    embedding2 = get_embedding(sentence2)  # Flatten the embedding array\n",
        "\n",
        "    # Convert to array\n",
        "    embedding1 = np.asarray(embedding1)\n",
        "    embedding2 = np.asarray(embedding2)\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = 1 - cosine(embedding1, embedding2)\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "AiaArDnaYBD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_sts_openai_score(ans_finetune, true_ans))\n",
        "print(calculate_sts_openai_score(ans_langchain, true_ans))\n",
        "print(calculate_sts_openai_score(ans_chatgpt, true_ans))\n",
        "print(calculate_sts_openai_score(ans_rag, true_ans))"
      ],
      "metadata": {
        "id": "6o__O2hSYSjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on Entire `.csv`"
      ],
      "metadata": {
        "id": "Y7JvopLcfu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HPaehA71gNZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #1: model_finetune\n",
        "    pred = model_finetune(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_1'] = current_ans"
      ],
      "metadata": {
        "id": "dEOoBrHUZOFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #2: call_langchain\n",
        "    try:\n",
        "        pred = call_langchain(query)\n",
        "    except:\n",
        "        pred = \"\"\n",
        "        print(\"Error\")\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_2'] = current_ans"
      ],
      "metadata": {
        "id": "2_jsFCo-hXTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #3: call_chatgpt\n",
        "    pred = call_chatgpt(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_3'] = current_ans"
      ],
      "metadata": {
        "id": "k4AWFidvhtm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #4: rag\n",
        "    pred = rag(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_4'] = current_ans"
      ],
      "metadata": {
        "id": "6DvfyUc3jGvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "current_data['score_approach_1'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_1'], x['answers']), axis=1)\n",
        "current_data['score_approach_2'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_2'], x['answers']), axis=1)\n",
        "current_data['score_approach_3'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_3'], x['answers']), axis=1)\n",
        "current_data['score_approach_4'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_4'], x['answers']), axis=1)"
      ],
      "metadata": {
        "id": "1ZtWjIfhjQWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_data.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/AI Research/Students/xxx/data/final_score_{nom}.csv\")"
      ],
      "metadata": {
        "id": "Pml01dd9jtel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR6QvRqzj6-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}