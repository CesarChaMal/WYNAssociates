{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvtPqYDYXemt"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCrkL4vg4m6"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai google-search-results chromadb pypdf sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeE8-wVchBLl"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI as l_OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsPQw_KXXgos"
      },
      "source": [
        "## Enter Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKAm6DMbg70L"
      },
      "outputs": [],
      "source": [
        "SERPAPI_API_KEY = \"xxx\"\n",
        "OPENAI_API_KEY = \"sk-xxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o21j5ejo6Cn"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE0wcqVtWCx8"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp-qjh5Po4QI"
      },
      "outputs": [],
      "source": [
        "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBINXpAmXiPt"
      },
      "source": [
        "## Approach 1\n",
        "\n",
        "This is API call to request fine tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xq0lIk5WxUh"
      },
      "outputs": [],
      "source": [
        "def model_finetune(query: str) -> str:\n",
        "    completion = openai_client.completions.create(\n",
        "        model=\"ft:davinci-002:personal::8JEsV0S6\", # fine tuned model using 12 csvs\n",
        "        prompt=query\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wti4HUIcXmK6"
      },
      "source": [
        "## Approach 2\n",
        "\n",
        "This is API call to ask `chatgpt` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsLFJz05WBJ7"
      },
      "outputs": [],
      "source": [
        "def call_chatgpt(query: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    \"\"\"\n",
        "    Generates a response to a query using the specified language model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query that needs to be processed.\n",
        "        model (str, optional): The language model to be used. Defaults to \"gpt-3.5-turbo\".\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response to the query.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare the conversation context with system and user messages.\n",
        "    messages: List[Dict[str, str]] = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {query}.\"},\n",
        "    ]\n",
        "\n",
        "    # Use the OpenAI client to generate a response based on the model and the conversation context.\n",
        "    response: Any = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    # Extract the content of the response from the first choice.\n",
        "    content: str = response.choices[0].message.content\n",
        "\n",
        "    # Return the generated content.\n",
        "    return content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00xMlUUGXsaK"
      },
      "source": [
        "## Approach 3\n",
        "\n",
        "This is to use `langchain` to have internet access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNAUcv-Ig4Aa"
      },
      "outputs": [],
      "source": [
        "def call_langchain(prompt: str) -> str:\n",
        "    llm = l_OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm, serpapi_api_key=SERPAPI_API_KEY)\n",
        "    agent = initialize_agent(\n",
        "        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        "    )\n",
        "    output = agent.run(prompt)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZxViUAIXxyl"
      },
      "source": [
        "## Approach 4\n",
        "\n",
        "Here we use RAG.\n",
        "\n",
        "PDF -> Chroma DB -> Vector DB -> A list numbers and document IDs\n",
        "\n",
        "User asks: query\n",
        "\n",
        "`chroma_collection.query`: This function will give you the relevant text about query based on the Vector DB. Results: -> A list of relevant documents and their numerical form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEIM7It7L_R2"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"/path/to/file/file_name.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgQMSlXbbMc7"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TdvnQppNEXB"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/AI Research/Students/xxx/lectures/2024\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirZEfIRMy2z"
      },
      "outputs": [],
      "source": [
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "from helper_utils import load_chroma, word_wrap, project_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pnG0jh-4NxJK",
        "outputId": "13a223de-880c-4e56-c987-b2e0f63c6785"
      },
      "outputs": [],
      "source": [
        "pdf_path.split('/')[-1].split('.')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBQBt4a1Mt_s",
        "outputId": "53f8ecdc-d940-4eb8-a657-8818116dfdcf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "embedding_function = SentenceTransformerEmbeddingFunction()\n",
        "\n",
        "nom = pdf_path.split('/')[-1].split('.')[0]\n",
        "chroma_collection = load_chroma(filename=pdf_path, collection_name=f'{nom}', embedding_function=embedding_function)\n",
        "chroma_collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYCAhJAlXz-d"
      },
      "outputs": [],
      "source": [
        "def rag(query: str) -> str:\n",
        "    results = chroma_collection.query(query_texts=query, n_results=10, include=['documents', 'embeddings'])\n",
        "    retrieved_doc = results['documents'][0]\n",
        "\n",
        "    updated_query = f\"\"\"\n",
        "        Answer the question: {query}\n",
        "        Based on the document provided: {retrieved_doc}\n",
        "    \"\"\"\n",
        "    response = call_chatgpt(updated_query)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT07CJkMa5zC"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "This assumes we get `.csv` per topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DGTaEmjyMvu",
        "outputId": "8dff867a-b1c0-4126-e225-874c3606a158"
      },
      "outputs": [],
      "source": [
        "print(nom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDtBUyxa8lI"
      },
      "outputs": [],
      "source": [
        "path_of_csv = f\"file/path/{nom}.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFkAtcFEcn56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iie3oLVbcqVB"
      },
      "outputs": [],
      "source": [
        "current_data = pd.read_csv(path_of_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "1Gecefmfcsdt",
        "outputId": "172037f2-9871-4992-fee9-52c4b3348c81"
      },
      "outputs": [],
      "source": [
        "current_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAIFDyb2Xw2j"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWlMK-zuWiFK"
      },
      "outputs": [],
      "source": [
        "query = current_data.questions[0]\n",
        "true_ans = current_data.answers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-JPYH-9dXQau",
        "outputId": "fd7bf13d-912b-42a9-e6b3-03abaffb9c23"
      },
      "outputs": [],
      "source": [
        "ans_finetune = model_finetune(query)\n",
        "ans_finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwj9p39_hOmc",
        "outputId": "15185026-ec02-4453-ef5f-935d41e71942"
      },
      "outputs": [],
      "source": [
        "ans_langchain = call_langchain(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "xVWuvRnvWfLp",
        "outputId": "3b5bb84c-665d-46fc-e6cf-70f8d7a4e236"
      },
      "outputs": [],
      "source": [
        "ans_langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "KIo15qXahPZs",
        "outputId": "e4b52943-f049-4a82-fa43-9517a9f8db7b"
      },
      "outputs": [],
      "source": [
        "ans_chatgpt = call_chatgpt(query)\n",
        "ans_chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "iZlOpcc6Wgn2",
        "outputId": "7d6c06a2-b53e-4c23-b0ac-19fe425dc405"
      },
      "outputs": [],
      "source": [
        "ans_rag = rag(query)\n",
        "ans_rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29r_qsMoX_8q"
      },
      "source": [
        "## Measure it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCKoflCFYZ1l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgJ5PBXYW5kk"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return openai_client.embeddings.create(input = [text], model=model).data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiaArDnaYBD7"
      },
      "outputs": [],
      "source": [
        "def calculate_sts_openai_score(sentence1: str, sentence2: str) -> float:\n",
        "    # Compute sentence embeddings\n",
        "    embedding1 = get_embedding(sentence1)  # Flatten the embedding array\n",
        "    embedding2 = get_embedding(sentence2)  # Flatten the embedding array\n",
        "\n",
        "    # Convert to array\n",
        "    embedding1 = np.asarray(embedding1)\n",
        "    embedding2 = np.asarray(embedding2)\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = 1 - cosine(embedding1, embedding2)\n",
        "\n",
        "    return similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o__O2hSYSjy",
        "outputId": "3da78741-9f4f-4583-acd3-4148cf9feab9"
      },
      "outputs": [],
      "source": [
        "print(calculate_sts_openai_score(ans_finetune, true_ans))\n",
        "print(calculate_sts_openai_score(ans_langchain, true_ans))\n",
        "print(calculate_sts_openai_score(ans_chatgpt, true_ans))\n",
        "print(calculate_sts_openai_score(ans_rag, true_ans))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7JvopLcfu7X"
      },
      "source": [
        "## Test on Entire `.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPaehA71gNZX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEOoBrHUZOFp",
        "outputId": "34b3b537-3c2f-4329-af62-4c10c3e81a36"
      },
      "outputs": [],
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #1: model_finetune\n",
        "    pred = model_finetune(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_1'] = current_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_jsFCo-hXTX"
      },
      "outputs": [],
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #2: call_langchain\n",
        "    try:\n",
        "        pred = call_langchain(query)\n",
        "    except:\n",
        "        pred = \"\"\n",
        "        print(\"Error\")\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_2'] = current_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4AWFidvhtm4",
        "outputId": "2c230f4c-12c0-4423-f455-b486b423e589"
      },
      "outputs": [],
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #3: call_chatgpt\n",
        "    pred = call_chatgpt(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_3'] = current_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DvfyUc3jGvi",
        "outputId": "39667fe2-e065-4c00-b9cf-823204026b73"
      },
      "outputs": [],
      "source": [
        "current_ans = []\n",
        "\n",
        "for i in tqdm(range(len(current_data))):\n",
        "    query = current_data.questions[i]\n",
        "\n",
        "    # Approach #4: rag\n",
        "    pred = rag(query)\n",
        "    current_ans.append(pred)\n",
        "\n",
        "current_data['approach_4'] = current_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZtWjIfhjQWi",
        "outputId": "d1d575b5-9d6b-46cd-907a-2483d23e3c9f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "current_data['score_approach_1'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_1'], x['answers']), axis=1)\n",
        "current_data['score_approach_2'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_2'], x['answers']), axis=1)\n",
        "current_data['score_approach_3'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_3'], x['answers']), axis=1)\n",
        "current_data['score_approach_4'] = current_data.apply(lambda x: calculate_sts_openai_score(x['approach_4'], x['answers']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pml01dd9jtel"
      },
      "outputs": [],
      "source": [
        "current_data.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/AI Research/Students/xxx/data/final_score_{nom}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR6QvRqzj6-E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
