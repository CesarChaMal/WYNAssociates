{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP8bMibI5iuuO75nSP6MuuH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Proximal Policy Optimization (PPO) with TensorFlow\n","\n","Understanding PPO reinforcement learning algorithm and implementing it with TensorFlow 2.x\n","\n","![image](https://miro.medium.com/max/1400/1*XreRjuz6MmATuoRvmprsdA.webp)\n","\n","The source of the notes come from this [blog](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-tensorflow-2-x-89c9430ecc26).\n","\n","In this article, we will try to understand Open-AI’s Proximal Policy Optimization algorithm for reinforcement learning. After some basic theory, we will be implementing PPO with TensorFlow 2.x. Before you read further, I would recommend you take a look at the Actor-Critic method from [here](https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-2of-2-b8ceb7e059db), as we will be modifying the code of that article for PPO.\n","\n"],"metadata":{"id":"DqQ4zNKh7fCR"}},{"cell_type":"markdown","source":["## Why PPO?\n","\n","1. *Unstable Policy Update*: In Many Policy Gradient Methods, policy updates are unstable because of larger step size, which leads to bad policy updates and when this new bad policy is used for learning then it leads to even worse policy. And if steps are small then it leads to slower learning.\n","\n","2. *Data Inefficiency*: Many learning methods learn from current experience and discard the experiences after gradient updates. This makes the learning process slow as a neural net takes lots of data to learn.\n","\n","PPO comes handy to overcome the above issues.\n","\n","### Core Idea Behind PPO\n","\n","In earlier Policy gradient methods, the objective function was something like $\\hat{\\mathbb{E}}[\\log \\pi_{\\theta}(a_t/s_t) \\cdot \\hat{A}_t]$. But now instead of the log of current policy, we will be taking the ratio of current policy and old policy.\n","\n","$$\\hat{\\mathbb{E}} \\big[\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} \\hat{A}_t \\big] = \\hat{\\mathbb{E}} \\big[r_t(\\theta) \\hat{A}_t \\big]$$\n","\n","Equation comes from this [paper](https://arxiv.org/abs/1707.06347).\n","\n","We will be also clipping the ratio and will the minimum of the two i.e b/w clipped and unclipped.\n","\n","$$L^{\\text{CLIP}(\\theta)} = \\hat{\\mathbb{E}} \\big[\\min(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\big]$$\n","\n","This clipped objective will restrict large policy updates as shown below.\n","\n","![image](https://miro.medium.com/max/1400/1*VN01Obh5VyJ6QuA0qfyq6w.webp)\n","\n","Photo from this [paper](https://arxiv.org/abs/1707.06347)\n"],"metadata":{"id":"cm-keDbJ7vdq"}},{"cell_type":"markdown","source":["## Algorithm Steps\n","\n","1. Play game for n steps and store state, action probability, rewards, done variables.\n","2. Apply the Generalized Advantage Estimation method on the above experience. We will see this in the coding section.\n","3. Train neural networks for some epochs by calculating their respective loss.\n","4. Test this trained model for “m” episodes.\n","5. If the average reward of test episodes is larger than the target reward set by you then stop otherwise repeat from step one."],"metadata":{"id":"R85BryFj9ONY"}},{"cell_type":"markdown","source":["## Code\n","\n","1. After importing the required libraries and initializing our environment, we define our neural networks and are similar to that of the Actor-Critic article.\n","2. The Actor-network takes the current state as input and outputs probability for each action.\n","3. The Critic network outputs the value of a state.\n","\n","```py\n","class critic():\n","    # here is a neural network with dense layers\n","\n","class actor():\n","    # here is another neural network with dense layers\n","```"],"metadata":{"id":"L5LFkLfq9ZNF"}},{"cell_type":"markdown","source":["## Action Selection\n","\n","1. We define our agent class and initialize optimizer and learning rate.\n","2. We also define a clip_pram variable which will be used in the actor loss function.\n","3. For action selection, we will be using the TensorFlow probabilities library, which takes probabilities as input and convert them into distribution.\n","4. Then, we use the distribution for action selection.\n","\n","```py\n","class agent()\n","    def __init__(self):\n","        # define optimizer\n","        # define actor()\n","        # define critic()\n","    \n","    def act(self, state):\n","        # define what the actor does\n","    \n","    def actor_loss(self, prob, action, td):\n","        # define the formuls\n","        # and compute the loss for actor\n","        # loss is defined according to PPO formula\n","    \n","    def learn():\n","        # use gradient tape to update gradient\n","        # according to PPO loss function\n","```"],"metadata":{"id":"mk3HBlzr9uDq"}},{"cell_type":"markdown","source":["## Test Model Knolwedge\n","\n","This function will be used to test our agent’s knowledge and returns the total reward for one episode.\n","\n","```py\n","def test_reward(env):\n","    # there we have a while-loop to update reward\n","```"],"metadata":{"id":"I4J_7zzL-Bsd"}},{"cell_type":"markdown","source":["## Training Loop\n","\n","1. We will loop for “steps” time i.e we will collect experience for “steps” time.\n","2. The next loop is for the number of times agent interacts with environments and we store experiences in different lists.\n","3. After the above loop, we calculate and add the value of the state next to the last state for calculations in the Generalized Advantage Estimation method.\n","4. Then, we process all the lists in the Generalized Advantage Estimation method to get returns, advantage.\n","5. Next, we train our networks for 10 epochs.\n","6. After training, we will test our agent on the test environment for five episodes.\n","7. If the average reward of test episodes is larger than the target reward set by you then stop otherwise repeat from step one.\n","\n","```py\n","# define params\n","for s in range(steps):\n","    # define params for inner loop\n","    while loop is running:\n","        # run\n","        agent.learn()\n","```"],"metadata":{"id":"tV2hpfEf-Lej"}},{"cell_type":"markdown","source":["## Code Starts from Here"],"metadata":{"id":"L5voX6Hj_ZT-"}},{"cell_type":"markdown","source":["### Library"],"metadata":{"id":"ShYHZIh7_dmk"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf \n","import gym\n","import tensorflow_probability as tfp"],"metadata":{"id":"xYIvgSYn5hUc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Installation\n","\n","Please install the following."],"metadata":{"id":"n6F2yJXI_f3i"}},{"cell_type":"code","source":["pip install box2d-py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2UxFdDS6YdT","executionInfo":{"status":"ok","timestamp":1674935218336,"user_tz":300,"elapsed":65594,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"0e183db7-ebc8-47db-db4a-b02cf5ace33f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp38-cp38-linux_x86_64.whl size=2835001 sha256=1aac00046f2e9a376195a72d2aa31ee39ec050a0df36c3e51c1d5107bf61c9d0\n","  Stored in directory: /root/.cache/pip/wheels/cc/4f/d6/44eb0a9e6fea384e58f19cb0c4125e46a23af2b33fe3a7e81c\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}]},{"cell_type":"code","source":["pip install gym[box2d]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV1JyU3u6VmG","executionInfo":{"status":"ok","timestamp":1674935287118,"user_tz":300,"elapsed":68786,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"9da39afe-d59a-4bf0-e396-dd61b910cdbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.0)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.1.1)\n","Collecting pygame==2.1.0\n","  Using cached pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","Collecting box2d-py==2.3.5\n","  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp38-cp38-linux_x86_64.whl size=2834814 sha256=130ff3883c95bb2e24e10d15454a08019dc496fcd2aa4e4f5e069e4a2325d292\n","  Stored in directory: /root/.cache/pip/wheels/8b/95/16/1dc99ff9a3f316ff245fdb5c9086cd13c35dad630809909075\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"]}]},{"cell_type":"markdown","source":["### Initiate Environment"],"metadata":{"id":"CaGSFskn_jh0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQly4cTp5VjL","executionInfo":{"status":"ok","timestamp":1674935287360,"user_tz":300,"elapsed":247,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"8d3f1978-1fa0-405f-d969-5b09211387d2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make(\"LunarLander-v2\")\n","low = env.observation_space.low\n","high = env.observation_space.high"]},{"cell_type":"markdown","source":["### Define Class Object: `critic`"],"metadata":{"id":"ony4paV9_m2T"}},{"cell_type":"code","source":["class critic(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.d1 = tf.keras.layers.Dense(2048,activation='relu')\n","        self.d2 = tf.keras.layers.Dense(1536,activation='relu')\n","        self.v = tf.keras.layers.Dense(1, activation = None)\n","\n","    def call(self, input_data):\n","        x = self.d1(input_data)\n","        x = self.d2(x)\n","        v = self.v(x)\n","        return v\n","\n","class actor(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.d1 = tf.keras.layers.Dense(2048,activation='relu')\n","        self.d2 = tf.keras.layers.Dense(1536,activation='relu')\n","        self.a = tf.keras.layers.Dense(4,activation='softmax')\n","\n","    def call(self, input_data):\n","        x = self.d1(input_data)\n","        x = self.d2(x)\n","        a = self.a(x)\n","        return a"],"metadata":{"id":"2Ca4f7_f5ik7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define Class Object: `agent`"],"metadata":{"id":"h16OUE1o_uan"}},{"cell_type":"code","source":["class agent():\n","    def __init__(self, gamma = 0.99):\n","        self.gamma = gamma\n","        self.a_opt = tf.keras.optimizers.Adam(learning_rate=5e-6)\n","        self.c_opt = tf.keras.optimizers.Adam(learning_rate=5e-6)\n","        self.actor = actor()\n","        self.critic = critic()\n","        self.log_prob = None\n","\n","    def act(self,state):\n","        prob = self.actor(np.array([state]))\n","        #print(prob)\n","        prob = prob.numpy()\n","        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n","        action = dist.sample()\n","        return int(action.numpy()[0])\n","        # action = np.random.choice([i for i in range(env.action_space.n)], 1, p=prob[0])\n","        # log_prob = tf.math.log(prob[0][action]).numpy()\n","        # self.log_prob = log_prob[0]\n","        # #print(self.log_prob)\n","        # return action[0]\n","\n","    def actor_loss(self, prob, action, td):\n","        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n","        log_prob = dist.log_prob(action)\n","        loss = -log_prob*td\n","        return loss\n","\n","    def learn(self, state, action, reward, next_state, done):\n","        state = np.array([state])\n","        next_state = np.array([next_state])\n","        #self.gamma = tf.convert_to_tensor(0.99, dtype=tf.double)\n","        #d = 1 - done\n","        #d = tf.convert_to_tensor(d, dtype=tf.double)\n","        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n","            p = self.actor(state, training=True)\n","                \n","            #p = self.actor(state, training=True).numpy()[0][action]\n","            #p = tf.convert_to_tensor([[p]], dtype=tf.float32)\n","            #print(p)\n","            v =  self.critic(state,training=True)\n","            #v = tf.dtypes.cast(v, tf.double)\n","\n","            vn = self.critic(next_state, training=True)\n","            #vn = tf.dtypes.cast(vn, tf.double)\n","            td = reward + self.gamma*vn*(1-int(done)) - v\n","            #print(td)\n","            #td = tf.math.subtract(tf.math.add(reward, tf.math.multiply(tf.math.multiply(self.gamma, vn), d)), v)\n","            #a_loss = -self.log_prob*td\n","            a_loss = self.actor_loss(p, action, td)\n","            #a_loss = -tf.math.multiply(tf.math.log(p),td)\n","            #a_loss = tf.keras.losses.categorical_crossentropy(td, p)\n","            #a_loss = -tf.math.multiply(self.log_prob,td)\n","            c_loss = td**2\n","            #c_loss = tf.math.pow(td,2)\n","        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n","        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n","        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n","        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n","        return a_loss, c_loss"],"metadata":{"id":"86hwMCju5nSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define and Run Training"],"metadata":{"id":"mnwj-IYm_wHi"}},{"cell_type":"code","source":["agentoo7 = agent()\n","steps = 10\n","\n","for s in range(steps):\n","\n","    done = False\n","    state = env.reset()\n","    total_reward = 0\n","    all_aloss = []\n","    all_closs = []\n","  \n","    while not done:\n","        #env.render()\n","        action = agentoo7.act(state)\n","        #print(action)\n","        next_state, reward, done, _ = env.step(action)\n","        aloss, closs = agentoo7.learn(state, action, reward, next_state, done)\n","        all_aloss.append(aloss)\n","        all_closs.append(closs)\n","        state = next_state\n","        total_reward += reward\n","\n","        if done:\n","            #print(\"total step for this episord are {}\".format(t))\n","            print(\"total reward after {} steps is {}\".format(s, total_reward))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybfQbu-s56sR","executionInfo":{"status":"ok","timestamp":1674935460992,"user_tz":300,"elapsed":92549,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"db4d374d-7e87-431e-dbe5-2d78d1ddbaef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total reward after 0 steps is -101.85410305615486\n","total reward after 1 steps is -388.9730642679365\n","total reward after 2 steps is -128.5784935695012\n","total reward after 3 steps is -94.56507968972281\n","total reward after 4 steps is -184.29354193471238\n","total reward after 5 steps is -420.0412933497794\n","total reward after 6 steps is -92.74203850219722\n","total reward after 7 steps is -156.6542665888134\n","total reward after 8 steps is -196.48567587771737\n","total reward after 9 steps is -213.73151792177583\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RtAsoiNg7XnF"},"execution_count":null,"outputs":[]}]}