{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPrq93rfbRx8SMqZOGV1OWy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What is an actor-critic model?\n","\n","<p align='center'><img src=\"https://i.imgur.com/5gCs5kH.gif\"></img></p>\n","\n","An actor-critic model is a type of reinforcement learning algorithm that combines the strengths of both value-based and policy-based approaches. It is designed to address the exploration-exploitation trade-off in reinforcement learning and improve learning stability and convergence.\n","\n","<p align='center'>\n","    <img src=\"https://i.stack.imgur.com/ir74w.png\"></img>\n","</p>\n","\n","The actor-critic model consists of two separate components:\n","\n","1. Actor: The actor is a policy-based model that learns to take actions in a given state based on a stochastic policy, which is a probability distribution over actions. The goal of the actor is to find the optimal policy, which maximizes the expected cumulative reward.\n","\n","2. Critic: The critic is a value-based model that learns to estimate the value function (e.g., state-value function V(s) or action-value function Q(s, a)). The value function represents the expected cumulative reward from a given state or state-action pair, following the policy learned by the actor.\n","\n","The actor and critic work together during the learning process. The critic evaluates the current policy by estimating the value function, while the actor updates its policy based on the feedback provided by the critic. This combination helps the algorithm to explore the environment efficiently, balance exploration and exploitation, and learn a more stable policy.\n","\n","Actor-critic methods can be implemented using various function approximators, such as neural networks, which can be trained using gradient-based optimization algorithms like stochastic gradient descent. These methods have been widely used in reinforcement learning applications, including robotics, game playing, and control systems.\n","\n","\n","\n"],"metadata":{"id":"NgClA9J9ndZc"}},{"cell_type":"markdown","source":["## Feed-forward Models: Actor and Critic\n","\n","There are the following components:\n"],"metadata":{"id":"Qpjhw5UxoLpV"}},{"cell_type":"markdown","source":["\n","### Actor\n","\n","- `build_actor_network`: The `build_actor_network` function in the provided code is responsible for constructing the actor part of the actor-critic model. The actor is a neural network that learns a policy, which is a probability distribution over actions given the current state. This function defines the architecture of the actor neural network and compiles it using an optimizer and a loss function.\n","\n","    - `inputs = Input(shape=(self.n_states,))`: This line defines the input layer of the neural network. It takes a state input with shape `(self.n_states,)`, where `self.n_states` is the number of features in the state representation.\n","\n","    - `x = Dense(16, activation='relu')(inputs)`: This line adds a fully connected `(dense)` layer with 16 hidden units and `ReLU` (Rectified Linear Unit) activation function. This layer processes the input state and learns intermediate representations.\n","\n","    - `x = Dense(16, activation='relu')(x)`: This line adds another fully connected layer with 16 hidden units and ReLU activation function, further processing the intermediate representations.\n","\n","    - `output = Dense(self.n_actions, activation='softmax')(x)`: This line defines the output layer of the actor network. It has as many output units as there are actions, denoted by `self.n_actions`. The output layer uses a `softmax` activation function, which produces a probability distribution over actions. This probability distribution represents the policy that the actor learns to follow.\n","\n","    - `model = Model(inputs=inputs, outputs=output)`: This line creates a Keras Model object with the specified input and output layers. The model represents the actor neural network.\n","\n","    The `build_actor_network` function returns the compiled actor model, which can be used to predict action probabilities for a given state and update its weights during training.\n"],"metadata":{"id":"u4iPx63x_iWp"}},{"cell_type":"markdown","source":["\n","### Critic\n","\n","- `build_critic_network`: The function in the provided code is responsible for constructing the critic part of the actor-critic model. The critic is a neural network that learns to estimate the value function (e.g., state-value function V(s) or action-value function Q(s, a)) for the policy learned by the actor. This function defines the architecture of the critic neural network and compiles it using an optimizer and a loss function.\n","\n","    - `inputs = Input(shape=(self.n_states,))`: This line defines the input layer of the neural network. It takes a state input with shape `(self.n_states,)`, where `self.n_states` is the number of features in the state representation.\n","\n","    - `x = Dense(16, activation='relu')(inputs)`: This line adds a fully connected `(dense)` layer with 16 hidden units and `ReLU` (Rectified Linear Unit) activation function. This layer processes the input state and learns intermediate representations.\n","\n","    - `x = Dense(16, activation='relu')(x)`: This line adds another fully connected layer with 16 hidden units and ReLU activation function, further processing the intermediate representations.\n","\n","    - `output = Dense(1, activation='linear')(x)`: This line defines the output layer of the critic network. It has a single output unit with a linear activation function, which produces the estimated value function for the given input state.\n","\n","    - `model = Model(inputs=inputs, outputs=output)`: This line creates a Keras Model object with the specified input and output layers. The model represents the critic neural network.\n","\n","    - `model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss='mean_squared_error')`: This line compiles the model using the Adam optimizer with a learning rate specified by the `lr` parameter. The loss function used is mean squared error, which is suitable for training a neural network to perform regression tasks, such as estimating the value function.\n","\n","    The `build_critic_network` function returns the compiled critic model, which can be used to predict the value function for a given state and update its weights during training. The critic provides feedback to the actor, helping it improve its policy by identifying which actions lead to higher expected cumulative rewards.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"dVEyN-gx_j_4"}},{"cell_type":"markdown","source":["## Exploration-Exploitation\n","\n","There are a few more functions. Let us discuss them below.\n","\n","- The `get_action(self, state)` function is a part of the ActorCritic class, and its purpose is to use the actor model to select an action based on the current state. This function takes the current state as input and returns the selected action according to the policy learned by the actor.\n","\n","    - `probabilities = self.actor.predict(state[np.newaxis])[0]`: This line uses the actor model to predict the action probabilities for the given state. Since the model expects input in the form of a batch, the state is wrapped in a singleton batch using `state[np.newaxis]`. The `[0]` index extracts the probabilities for the first (and only) element in the batch.\n","\n","    - `action = np.random.choice(self.n_actions, p=probabilities)`: This line selects an action according to the probability distribution over actions produced by the actor. It uses the np.random.choice function, which randomly samples an element from the given range (0 to self.n_actions - 1) with the specified probabilities. The resulting action follows the policy learned by the actor.\n","\n","    The `get_action(self, state)` function returns the selected action, which can be used in the environment to execute the action and observe the next state, reward, and done flag. This function is called during the interaction with the environment, both during training and when using the trained actor-critic model for decision-making.\n","\n","    The `get_action(self, state)` function plays a crucial role in the exploration-exploitation trade-off, as it allows the agent to explore different actions based on the probability distribution produced by the actor. By following the learned policy, the agent can balance exploration (trying out new actions) and exploitation (choosing actions that have led to higher rewards in the past), enabling it to learn more effective policies over time.\n","\n"],"metadata":{"id":"cDKMlbfSx6xB"}},{"cell_type":"markdown","source":["## Training\n","\n","The last piece of puzzle is the `train` function.\n","\n","- The `train(self, state, action, reward, next_state, done)` function in the ActorCritic class is responsible for updating the actor and critic models based on the experience collected during the agent's interaction with the environment. The function takes the current state, action, reward, next state, and a boolean flag done indicating whether the episode has ended.\n","\n","    - `target = reward`: This line initializes the target value for the critic with the immediate reward received after taking the action.\n","\n","    - `if not done: target += self.critic.predict(next_state[np.newaxis])[0][0]`: If the episode has not ended, the function adds the estimated value of the next state to the target. This is obtained by feeding the next state to the critic model and extracting the estimated value from the prediction. This step is based on the concept of bootstrapping, which incorporates the critic's estimate of future rewards into the target value.\n","\n","    - `td_error = target - self.critic.predict(state[np.newaxis])[0][0]`: This line calculates the temporal difference (TD) error, which is the difference between the target value and the critic's current estimate of the state value. The TD error serves as a learning signal for both the actor and the critic.\n","\n","    - `target_vec = self.critic.predict(state[np.newaxis])`: This line predicts the current state's value using the critic model and stores it in target_vec. This is done to create a target vector for training the critic with the same shape as the critic's output.\n","\n","    - `target_vec[0][0] = target`: This line updates the first element of target_vec with the calculated target value. This target value now includes the immediate reward and, if the episode has not ended, the estimated value of the next state.\n","\n","    - `self.critic.train_on_batch(state[np.newaxis], target_vec)`: This line trains the critic model using the train_on_batch method, which performs a single gradient update on the given state and target vector. This step updates the critic's weights to reduce the difference between its predicted state value and the target value.\n","\n","    - `advantages = np.zeros((1, self.n_actions))`: This line initializes an \"advantages\" vector with the same shape as the actor's output. This vector will be used to train the actor by providing the TD error as a learning signal for the chosen action.\n","\n","    - `advantages[0][action] = td_error`: This line sets the advantage of the chosen action in the advantages vector to the calculated TD error. The advantages of other actions remain zero, so their probabilities won't be updated during training.\n","\n","    - `self.actor.train_on_batch(state[np.newaxis], advantages)`: This line trains the actor model using the train_on_batch method with the given state and advantages vector. This step updates the actor's weights to maximize the probability of the chosen action, guided by the TD error provided by the critic.\n","\n","    The `train` function updates both the actor and critic models using the TD error as a learning signal. This enables the actor to improve its policy, while the critic refines its value function estimates, resulting in better decision-making and more effective exploration and exploitation.\n","\n","\n","\n"],"metadata":{"id":"JoIfSgqK71z5"}},{"cell_type":"markdown","source":["## Library"],"metadata":{"id":"AColOVSTFXr_"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"metadata":{"id":"SXlh95G0A6NG","executionInfo":{"status":"ok","timestamp":1679180117117,"user_tz":240,"elapsed":162,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["class ActorCritic:\n","    def __init__(self, n_states, n_actions, actor_lr=0.001, critic_lr=0.01):\n","        self.n_states = n_states\n","        self.n_actions = n_actions\n","\n","        self.actor = self.build_actor_network(actor_lr)\n","        self.critic = self.build_critic_network(critic_lr)\n","\n","    def build_actor_network(self, lr):\n","        inputs = Input(shape=(self.n_states,))\n","        x = Dense(16, activation='relu')(inputs)\n","        x = Dense(16, activation='relu')(x)\n","        output = Dense(self.n_actions, activation='softmax')(x)\n","        model = Model(inputs=inputs, outputs=output)\n","        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy')\n","        return model\n","\n","    def build_critic_network(self, lr):\n","        inputs = Input(shape=(self.n_states,))\n","        x = Dense(16, activation='relu')(inputs)\n","        x = Dense(16, activation='relu')(x)\n","        output = Dense(1, activation='linear')(x)\n","        model = Model(inputs=inputs, outputs=output)\n","        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_squared_error')\n","        return model\n","\n","    def get_action(self, state):\n","        probabilities = self.actor.predict(state[np.newaxis], verbose=0)[0]\n","        action = np.random.choice(self.n_actions, p=probabilities)\n","        return action\n","\n","    def train(self, state, action, reward, next_state, done):\n","        target = reward\n","        if not done:\n","            target += self.critic.predict(next_state[np.newaxis], verbose=0)[0][0]\n","\n","        td_error = target - self.critic.predict(state[np.newaxis], verbose=0)[0][0]\n","        target_vec = self.critic.predict(state[np.newaxis], verbose=0)\n","        target_vec[0][0] = target\n","        self.critic.train_on_batch(state[np.newaxis], target_vec)\n","\n","        advantages = np.zeros((1, self.n_actions))\n","        advantages[0][action] = td_error\n","        self.actor.train_on_batch(state[np.newaxis], advantages)\n","\n","        return state, action, reward, next_state, done\n"],"metadata":{"id":"BlPoTE4emis4","executionInfo":{"status":"ok","timestamp":1679180119120,"user_tz":240,"elapsed":177,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["## Generate Random Data"],"metadata":{"id":"qQxgRBquFZJV"}},{"cell_type":"code","source":["# Environment parameters\n","n_states = 9\n","n_actions = 9\n","\n","# Generate fake data\n","n_samples = 100\n","states = np.random.randint(0, 2, size=(n_samples, n_states))\n","next_states = np.random.randint(0, 2, size=(n_samples, n_states))\n","rewards = np.random.randint(-1, 2, size=(n_samples,))\n","actions = np.random.randint(0, n_actions, size=(n_samples,))\n","dones = np.random.randint(0, 2, size=(n_samples,))\n","\n","# Dimension\n","print(f\"State has dim: {states.shape}\")\n","print(f\"Next State has dim: {next_states.shape}\")\n","print(f\"Reward has dim: {rewards.shape}\")\n","print(f\"Action has dim: {actions.shape}\")\n","print(f\"Dones has dim: {dones.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqzDVjaKmwfO","executionInfo":{"status":"ok","timestamp":1679188570830,"user_tz":240,"elapsed":172,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"847d1199-21c2-4b48-c114-6d870bbf3620"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["State has dim: (100, 9)\n","Next State has dim: (100, 9)\n","Reward has dim: (100,)\n","Action has dim: (100,)\n","Dones has dim: (100,)\n"]}]},{"cell_type":"markdown","source":["## Generate Realistic Data\n","\n","<p align='center'><img src=\"https://media.tenor.com/rH3Yh7-6UhsAAAAC/cereal-tic-tac-toe.gif\"></img></p>\n","\n","To generate a more realistic tic-tac-toe dataset with valid actions, rewards, and dones, we can simulate games and record the agent's experiences during gameplay. For simplicity, we will assume player 1 (X) and player 2 (O) take turns playing randomly. Here's the code to generate such a dataset:\n","\n","In this code, we first define a `check_winner()` function that checks if either player has won. Then, we loop through the desired number of games and simulate each game by alternating between players and selecting random valid actions. We store the states, next states, actions, rewards, and done flags for each step of the game. Finally, we convert these lists to NumPy arrays.\n","\n","This dataset represents a more realistic tic-tac-toe dataset that can be used to train a reinforcement learning model. Keep in mind that since the actions are selected randomly, the dataset's quality may not be optimal for learning an optimal strategy. However, it is a better starting point than a completely random dataset."],"metadata":{"id":"8IjEkLGrEprg"}},{"cell_type":"markdown","source":["### Check Winner Function\n","\n","The `check_winner()` function takes a 1D NumPy array representing a tic-tac-toe board and checks if either player 1 (X) or player 2 (O) has won. If a winner is found, the function returns the winner's number (1 or 2). If there is no winner, the function returns 0.\n","\n","Here's a breakdown of the function:\n","\n","- `lines`: This variable defines a list of 8 possible winning lines in a tic-tac-toe board. Each line represents a row, column, or diagonal. For example, `board[0:3]` represents the first row, `board[3:6]` represents the second row, `board[6:9]` represents the third row, and so on.\n","\n","- The `for` loop iterates through each line in the lines list.\n","\n","    a. `if np.all(line == 1)`: The `np.all()` function checks if all elements in the line array are equal to 1, which indicates that player 1 has a winning line. If this condition is met, the function returns 1.\n","\n","    b. `elif np.all(line == 2)`: Similar to the previous condition, this checks if all elements in the `line` array are equal to 2, which indicates that player 2 has a winning line. If this condition is met, the function returns 2.\n","\n","- If the `for` loop completes without finding a winner, the function returns 0, indicating that there is no winner in the current board state.\n","\n","The `check_winner()` function is a utility function used during the game simulation to determine if a player has won after making a move. This information is used to update the rewards and done flags in the dataset appropriately."],"metadata":{"id":"JyUCKBy5FYLa"}},{"cell_type":"markdown","source":["In the `check_winner()` function, we rely on `np.all()` function.\n","\n","`np.all()` is a function in the NumPy library that tests whether all elements in an input array (or along a specified axis) evaluate to True. It returns a single boolean value, True if all elements are True and False otherwise.\n","\n","```py\n","import numpy as np\n","\n","arr = np.array([True, True, True])\n","result = np.all(arr)\n","print(result)  # Output: True\n","\n","arr = np.array([True, False, True])\n","result = np.all(arr)\n","print(result)  # Output: False\n","```\n","\n","In the code provided below, `np.all(line == 1)` checks if all elements in the `line` array are equal to 1. Similarly, `np.all(line == 2)` checks if all elements in the `line` array are equal to 2. These checks are used to determine if a player has won the game by having a complete row, column, or diagonal with their respective number (1 or 2).\n","\n","\n","\n"],"metadata":{"id":"B4-hyo0GHSIH"}},{"cell_type":"code","source":["def check_winner(board):\n","    lines = [\n","        board[0:3], board[3:6], board[6:9],      # Rows\n","        board[0::3], board[1::3], board[2::3],   # Columns\n","        board[0::4], board[2:8:2]                # Diagonals\n","    ]\n","    \n","    for line in lines:\n","        if np.all(line == 1):\n","            return 1\n","        elif np.all(line == 2):\n","            return 2\n","    return 0\n"],"metadata":{"id":"1DAGQANOErOi","executionInfo":{"status":"ok","timestamp":1679180130533,"user_tz":240,"elapsed":160,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["1. The for loop iterates num_games times, which is set to 100. This means the code will simulate 100 games. This assumes if `num_games` is set to be 100 in the code below.\n","\n","```py\n","for _ in range(num_games):\n","```\n","\n","2. Initialize the `board` as a 1D numpy array of length 9 with all zeros, representing an empty Tic-Tac-Toe board. Set `player` to 1, representing the first player.\n","\n","```py\n","    board = np.zeros(9)\n","    player = 1\n","```\n","\n","3. The inner while loop runs until the game ends (either a player wins, or the board is full).\n","\n","```py\n","    while True:\n","```\n","\n","4. Find the valid actions (empty spots on the board) and choose a random action from them.\n","\n","```py\n","        valid_actions = np.where(board == 0)[0]\n","        action = np.random.choice(valid_actions)\n","```\n","\n","5. Create a copy of the current board and apply the chosen action by setting the corresponding cell to the current player's number (1 or 2).\n","\n","```py\n","        next_board = board.copy()\n","        next_board[action] = player\n","```\n","\n","6. Check if there is a winner or if the game has ended by analyzing the updated board.\n","\n","```py\n","        winner = check_winner(next_board)\n","        done = winner != 0 or len(valid_actions) == 1\n","```\n","\n","7. Calculate the reward based on the winner. If player 1 wins, the reward is 1. If player 2 wins, the reward is -1. If the game is a draw or hasn't ended yet, the reward is 0.\n","\n","```py\n","        reward = 0\n","        if winner == 1:\n","            reward = 1\n","        elif winner == 2:\n","            reward = -1\n","```\n","\n","8. Append the current board state, next state, action, reward, and done status to their respective lists.\n","\n","```py\n","        states.append(board)\n","        next_states.append(next_board)\n","        actions.append(action)\n","        rewards.append(reward)\n","        dones.append(done)\n","```\n","\n","9. If the game has ended, break out of the while loop.\n","\n","```py\n","        if done:\n","            break\n","```\n","\n","10. If the game hasn't ended, update the board to the next state and switch to the other player.\n","\n","```py\n","        board = next_board\n","        player = 3 - player  # Switch between 1 and 2\n","```\n","\n","After the for loop has completed, the code converts the lists to numpy arrays and prints the dimensions of the arrays.\n","\n"],"metadata":{"id":"TIAK7gNvGH0z"}},{"cell_type":"markdown","source":["### Create Data"],"metadata":{"id":"9hYHfOJa8NX7"}},{"cell_type":"code","source":["valid_actions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhOIKbMIb8lo","executionInfo":{"status":"ok","timestamp":1679188544708,"user_tz":240,"elapsed":160,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"50726cf3-b807-4d0a-bfac-306c1041cfb6"},"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7])"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","source":["# Environment parameters\n","n_states = 9\n","n_actions = 9\n","\n","states = []\n","next_states = []\n","actions = []\n","rewards = []\n","dones = []\n","\n","num_games = 10000\n","\n","for _ in range(num_games):\n","    board = np.zeros(9)\n","    player = 1\n","    \n","    while True:\n","        valid_actions = np.where(board == 0)[0]\n","        action = np.random.choice(valid_actions)\n","        \n","        next_board = board.copy()\n","        next_board[action] = player\n","        \n","        winner = check_winner(next_board)\n","        done = winner != 0 or len(valid_actions) == 1\n","        \n","        reward = 0\n","        if winner == 1:\n","            reward = 1\n","        elif winner == 2:\n","            reward = -1\n","            \n","        states.append(board)\n","        next_states.append(next_board)\n","        actions.append(action)\n","        rewards.append(reward)\n","        dones.append(done)\n","        \n","        if done:\n","            break\n","        \n","        board = next_board\n","        player = 3 - player  # Switch between 1 and 2\n","\n","states = np.array(states)\n","next_states = np.array(next_states)\n","actions = np.array(actions)\n","rewards = np.array(rewards)\n","dones = np.array(dones)\n","\n","# Dimension\n","print(f\"State has dim: {states.shape}\")\n","print(f\"Next State has dim: {next_states.shape}\")\n","print(f\"Reward has dim: {rewards.shape}\")\n","print(f\"Action has dim: {actions.shape}\")\n","print(f\"Dones has dim: {dones.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjEQyNWpGGS4","executionInfo":{"status":"ok","timestamp":1679180659706,"user_tz":240,"elapsed":11968,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"af164503-2653-4398-d8b3-ba6153c48cb5"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["State has dim: (76063, 9)\n","Next State has dim: (76063, 9)\n","Reward has dim: (76063,)\n","Action has dim: (76063,)\n","Dones has dim: (76063,)\n"]}]},{"cell_type":"markdown","source":["### Visualize Generated Training Data\n","\n","Let us use `matplotlib.pyplot` or also short for `plt` to visualize the data that we generated. It is fake data, but it is generated using certain rules. Let's visualize if it makes sense."],"metadata":{"id":"OHELvzC8ANwN"}},{"cell_type":"code","source":["# check one instance\n","i = 1\n","states[i], next_states[i], rewards[i], actions[i], dones[i]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuNty76uExLy","executionInfo":{"status":"ok","timestamp":1679180144008,"user_tz":240,"elapsed":184,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"a3a0472b-2e7e-4319-b6a9-cb9808190c23"},"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n"," array([0., 0., 0., 0., 0., 2., 1., 0., 0.]),\n"," 0,\n"," 5,\n"," False)"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["def plot_3x3_array(arr):\n","    if arr.shape != (3, 3):\n","        raise ValueError(\"Input array should be of shape (3, 3)\")\n","\n","    plt.figure(figsize=(5, 5))\n","    fig, ax = plt.subplots(3, 3, figsize=(5, 5))\n","    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","\n","    for i in range(3):\n","        for j in range(3):\n","            ax[i, j].text(0.5, 0.5, str(arr[i, j]), fontsize=14, ha='center', va='center')\n","            ax[i, j].axis('off')\n","\n","    plt.show()"],"metadata":{"id":"z8rqhfi5BQzp","executionInfo":{"status":"ok","timestamp":1679180146445,"user_tz":240,"elapsed":155,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["# start of a game out of many sequences of games\n","np.where(dones==True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6XSw42NmnCP","executionInfo":{"status":"ok","timestamp":1679180147632,"user_tz":240,"elapsed":153,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"3971dc34-1771-44b1-b1e2-eec6a15b8f01"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([  6,  15,  21,  30,  38,  44,  53,  62,  67,  72,  80,  89,  96,\n","        102, 111, 118, 125, 134, 141, 150, 158, 165, 172, 181, 186, 195,\n","        202, 207, 215, 222, 230, 235, 244, 253, 258, 266, 273, 282, 289,\n","        297, 306, 311, 320, 329, 338, 346, 355, 364, 373, 380, 388, 397,\n","        403, 412, 420, 427, 435, 444, 453, 462, 469, 478, 485, 491, 498,\n","        507, 515, 523, 531, 540, 549, 558, 567, 576, 585, 591, 598, 606,\n","        615, 622, 631, 640, 645, 653, 662, 669, 678, 683, 692, 698, 705,\n","        712, 721, 730, 738, 746, 754, 762, 767, 774]),)"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["i = 763\n","while True:\n","    print(f\"Step={i}; Game status:{dones[i]}\")\n","    plot_3x3_array(states[i].reshape((3, 3)))\n","    if dones[i] == True:\n","        break\n","    else:\n","        i += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0NfPLrqyAZVF","executionInfo":{"status":"ok","timestamp":1679180167976,"user_tz":240,"elapsed":3089,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"ea6ffeb9-fe85-44ce-d1c5-0baec96dbb13"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["Step=763; Game status:False\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Step=764; Game status:False\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAASUAAAEeCAYAAADM2gMZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAI50lEQVR4nO3av6tfdx3H8dfH/kCh6FJIcEjtGLBgaR1SSOOSf8AMWTukELMIipvUzdCtpWM7SCZnB6URISo0FJpOboGmSzWBTpLaNGk5DvcELl/yJbkY73mdex8P+EA553O+nPO+8Lw339MxTVMAWnxr6RsA2E2UgCqiBFQRJaCKKAFVRAmoIkpAFVECqogSUEWUgCqiBFQRJaCKKAFVRAmoIkpAFVECqogSUEWUgCqiBFQRJaCKKAFVRAmoIkpAFVECqogSUEWUgCqiBFQRJaCKKAFVRAmoIkpAFVECqogSUEWUgCqiBFQRJaCKKAFVRAmoIkpAFVECqogSUEWUgCqiBFRZVZTGGBfGGDfGGHfGGNfGGCcfsv/UvO/OGOOTMcb5/brXg8j8l3OoZj9N0ypWkrNJ7iV5PcnxJO8kuZ3k2Jb9zyf5Yt53fL7uXpIzSz/LGpf5m/2+Pe/SN7CHH8yHSd7dOHY9ycUt+99Mcn3j2HtJri79LGtc5m/2+7VW8c+3McbTSV5Kcnnj1OUkr2y57MQD9r+f5OUxxlOP9w4PNvNfzmGc/SqilOTZJE8kubVx/FaSo1uuObpl/5Pz5/HozH85h272a4kScEisJUqfJ/kmyZGN40eS3Nxyzc0t+7+eP49HZ/7LOXSzX0WUpmm6m+RaktMbp04n+WDLZVe37P9omqZ7j/cODzbzX86hnP3S37Tv4Q3E2SR3k5zLzmvOt7PzWvS5+fylJJd27b//WvStef+5+fpVvBZtW+Zv9vv2vEvfwB5/OBeSfJrkq+z89nh117krSa5s7D+V5ON5/40k55d+hjUv8zf7/VhjfgCACqv4Tgk4PEQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqLKqKI0xLowxbowx7owxro0xTj5k/6l5350xxidjjPP7da8Hkfkv51DNfpqmVawkZ5PcS/J6kuNJ3klyO8mxLfufT/LFvO/4fN29JGeWfpY1LvM3+3173qVvYA8/mA+TvLtx7HqSi1v2v5nk+sax95JcXfpZ1rjM3+z3a63in29jjKeTvJTk8sapy0le2XLZiQfsfz/Jy2OMpx7vHR5s5r+cwzj7VUQpybNJnkhya+P4rSRHt1xzdMv+J+fP49GZ/3IO3ezXEiXgkFhLlD5P8k2SIxvHjyS5ueWam1v2fz1/Ho/O/Jdz6Ga/iihN03Q3ybUkpzdOnU7ywZbLrm7Z/9E0Tfce7x0ebOa/nEM5+6W/ad/DG4izSe4mOZed15xvZ+e16HPz+UtJLu3af/+16Fvz/nPz9at4Ldq2zN/s9+15l76BPf5wLiT5NMlX2fnt8equc1eSXNnYfyrJx/P+G0nOL/0Ma17mb/b7scb8AAAVVvGdEnB4iBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKqIEVBEloIooAVVECagiSkAVUQKqiBJQRZSAKquK0hjjwhjjxhjjzhjj2hjj5EP2n5r33RljfDLGOL9f93oQmf8yxhivjjH+MMb4bIwxjTFee4RrXhhj/HWM8eV83RtjjLEPt/s/W02Uxhhnk7yd5LdJXkzyQZI/jTGObdn/fJI/zvteTHIxyTtjjDP7c8cHi/kv6pkk/0jy8yRfPmzzGOO7Sf6c5FaSH8/X/SrJL/6P9/j4TNO0ipXkwyTvbhy7nuTilv1vJrm+cey9JFeXfpY1LvPvWEluJ3ntIXt+luTfSb6z69ivk3yWZCz9DA9bq/hLaYzxdJKXklzeOHU5yStbLjvxgP3vJ3l5jPHU473Dg838V+dEkr9P07T7r6r3k3w/yQ8WuaM9WEWUkjyb5Ins/Dm6260kR7dcc3TL/ifnz+PRmf+6bJv9/XPV1hIl4JBYS5Q+T/JNkiMbx48kubnlmptb9n89fx6PzvzXZdvs75+rtoooTdN0N8m1JKc3Tp3OztudB7m6Zf9H0zTde7x3eLCZ/+pcTXJyjPHtXcdOJ/lnkk8XuaO9WPqb9j28dTib5G6Sc0mOZ+f19O0kz83nLyW5tGv/80m+SPLWvP/cfP2ZpZ9ljcv8F539M0l+NK//JHlj/u9j8/mLSf6ya//3svMX0e+T/DDJT7PzNu6XSz/LIz3v0jewxx/OheyU/qvs/OZ+dde5K0mubOw/leTjef+NJOeXfoY1L/NfbO4/STI9YP1uPv+7JJ9uXPNCkr8luZPkX0l+kxX87wDTNO3cJECLVXynBBweogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVUQJqCJKQBVRAqqIElBFlIAqogRUESWgiigBVf4LNGf+FwJnmQYAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Step=765; Game status:False\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Step=766; Game status:False\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Step=767; Game status:True\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["## Fitting the Data"],"metadata":{"id":"nGFggABolhQH"}},{"cell_type":"code","source":["# Initialize the actor-critic model\n","actor_critic = ActorCritic(n_states, n_actions)\n","\n","# Train the model using the fake data\n","all_results = []\n","for i in range(n_samples):\n","    state = states[i]\n","    action = actions[i]\n","    reward = rewards[i]\n","    next_state = next_states[i]\n","    done = bool(dones[i])\n","\n","    this_result = actor_critic.train(state, action, reward, next_state, done)\n","    all_results.append(this_result)\n","\n","    if i % 10 == 0:\n","        print(f\">> finished with sample: {i}/{n_samples}; current rewards={rewards[i]} <<\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qAF4t05mydE","executionInfo":{"status":"ok","timestamp":1679188650528,"user_tz":240,"elapsed":25121,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"9386805e-6460-4b5c-f4f9-9e533c2df193"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":[">> finished with sample: 0/100; current rewards=-1 <<\n",">> finished with sample: 10/100; current rewards=1 <<\n",">> finished with sample: 20/100; current rewards=1 <<\n",">> finished with sample: 30/100; current rewards=1 <<\n",">> finished with sample: 40/100; current rewards=0 <<\n",">> finished with sample: 50/100; current rewards=-1 <<\n",">> finished with sample: 60/100; current rewards=1 <<\n",">> finished with sample: 70/100; current rewards=1 <<\n",">> finished with sample: 80/100; current rewards=-1 <<\n",">> finished with sample: 90/100; current rewards=-1 <<\n"]}]},{"cell_type":"markdown","source":["## Prediction\n","\n","Let us check if the system works. Provided with a `test_state` the AI model should be able to make some guesses."],"metadata":{"id":"7Oa18ZQeli_K"}},{"cell_type":"code","source":["# Test the trained model\n","test_state = np.random.randint(0, 2, size=(n_states,))\n","action = actor_critic.get_action(test_state)\n","print(f\"Test state: {test_state}, Selected action: {action}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vh44FulgnF7y","executionInfo":{"status":"ok","timestamp":1679180562206,"user_tz":240,"elapsed":148,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"0d5fbf8c-7bb7-4b47-99f4-285c514b1d9d"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Test state: [0 0 1 0 1 0 1 1 1], Selected action: 6\n"]}]},{"cell_type":"markdown","source":["## Play: Let's play against this AI!\n","\n","We just trained a model called `actor_critic` to learn to play tic-tac-toe on simulated data. Let us try to set up a game to play with it."],"metadata":{"id":"7r3rf3x8nxbh"}},{"cell_type":"code","source":["test_state = np.array([\n","    0, 1, 0,\n","    0, 1, 2,\n","    0, 0, 0\n","])\n","plot_3x3_array(test_state.reshape((3, 3)))\n","action = actor_critic.get_action(test_state)\n","print(f\"Test state: {test_state}, Selected action: {action}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"-zKiCh3poF_e","executionInfo":{"status":"ok","timestamp":1679180620928,"user_tz":240,"elapsed":1317,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"5645758d-eb91-4b2c-90fe-042daa43dabc"},"execution_count":119,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 9 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Test state: [0 1 0 0 1 2 0 0 0], Selected action: 5\n"]}]},{"cell_type":"markdown","source":["Done."],"metadata":{"id":"3qqDR4hjhO0e"}}]}