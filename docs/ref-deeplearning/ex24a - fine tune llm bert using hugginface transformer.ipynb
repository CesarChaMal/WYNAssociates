{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training and fine-tuning\n",
        "\n",
        "Model classes in 🤗 Transformers are designed to be compatible with native PyTorch and TensorFlow 2 and can be used seemlessly with either. In this quickstart, we will show how to fine-tune (or train from scratch) a model using the standard training tools available in either framework. We will also show how to use our included `Trainer()` class which handles much of the complexity of training for you.\n",
        "\n",
        "This guide assume that you are already familiar with loading and use our models for inference; otherwise, see the [task summary](https://huggingface.co/transformers/v3.0.2/task_summary.html). We also assume that you are familiar with training deep neural networks in either PyTorch or TF2, and focus specifically on the nuances and tools for training models in 🤗 Transformers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wa5ruPpMrERy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning in native TensorFlow 2\n",
        "\n",
        "Models can also be trained natively in TensorFlow 2. Just as with PyTorch, TensorFlow models can be instantiated with `from_pretrained()` to load the weights of the encoder from a pretrained model."
      ],
      "metadata": {
        "id": "t8H91vK-ovNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us review the code before we continue.\n",
        "\n",
        "This code snippet is for setting up a machine learning model for sequence classification using TensorFlow and the BERT (Bidirectional Encoder Representations from Transformers) architecture. Here's what each line does:\n",
        "\n",
        "1. `model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')`\n",
        "   - This line initializes a BERT model for the task of sequence classification using the TensorFlow framework.\n",
        "   - `TFBertForSequenceClassification` is a class from the `transformers` library specifically designed for the task of classifying sequences (e.g., sentences or paragraphs) into categories.\n",
        "   - The `from_pretrained` method is used to load a pre-trained BERT model. In this case, `'bert-base-uncased'` refers to a BERT model that has been pre-trained on a large corpus of English data in an uncased format (i.e., the text has been converted to lowercase).\n",
        "\n",
        "2. `tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')`\n",
        "   - This line creates a tokenizer that will be used to convert text data into a format that can be fed into the BERT model.\n",
        "   - `BertTokenizer` is a class that provides tokenization for BERT models.\n",
        "   - The `from_pretrained` method is again used to load a tokenizer that is compatible with the `'bert-base-uncased'` model.\n",
        "\n",
        "3. `data = tfds.load('glue/mrpc')`\n",
        "   - This line loads a dataset using TensorFlow Datasets (`tfds`).\n",
        "   - `'glue/mrpc'` refers to the MRPC (Microsoft Research Paraphrase Corpus) task of the GLUE (General Language Understanding Evaluation) benchmark, which consists of sentence pairs labeled as either semantically equivalent or not.\n",
        "\n",
        "4. `train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')`\n",
        "   - This line converts the training examples from the loaded dataset into features that can be used by the model.\n",
        "   - `glue_convert_examples_to_features` is a utility function that processes the examples using the provided tokenizer, setting a maximum sequence length (`max_length=128`), and specifying the task (`task='mrpc'`) to ensure that the data is processed in a way that is suitable for the MRPC task.\n",
        "\n",
        "5. `train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)`\n",
        "   - This line shuffles the training dataset with a buffer size of 100 to introduce randomness in the order of examples.\n",
        "   - It then groups the data into batches of 32 examples each, which is a common practice to make training more efficient.\n",
        "   - Finally, the `repeat(2)` method is called to repeat the dataset for 2 epochs, meaning that the model will see the entire dataset twice during training.\n",
        "\n",
        "6. `test_dataset = glue_convert_examples_to_features(data['test'], tokenizer, max_length=128, task='mrpc')`\n",
        "   - This line is similar to line 4 but is applied to the test data. It processes the test examples into features in the same way as the training data.\n",
        "\n",
        "7. `test_dataset = test_dataset.shuffle(100).batch(32).repeat(2)`\n",
        "   - Similar to line 5, this line prepares the test dataset for evaluation by shuffling, batching, and repeating it. However, typically the test dataset should not be repeated as you usually only evaluate on the test set once. The repetition here might be an error or specific to some experimental setup.\n",
        "\n",
        "This code is typically used in the context of fine-tuning a pre-trained BERT model on a specific task, in this case, the MRPC task of the GLUE benchmark, and then evaluating its performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ln9GXAEUs_Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaTst0YBmVnG",
        "outputId": "9ec96412-be08-4d65-aa18-810c8d5af621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use `tensorflow_datasets` to load in the [MRPC dataset](https://www.tensorflow.org/datasets/catalog/glue#gluemrpc) from GLUE. We can then use our built-in `glue_convert_examples_to_features()` to tokenize MRPC and convert it to a TensorFlow `Dataset` object. Note that tokenizers are framework-agnostic, so there is no need to prepend `TF` to the pretrained tokenizer name.\n"
      ],
      "metadata": {
        "id": "yR-HZa1uo1jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, glue_convert_examples_to_features\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "CvcDjhc4mWP9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using the `glue/mrpc` dataset from TensorFlow Datasets (TFDS), the task at hand is to determine whether two sentences are semantically equivalent or not. This task is a binary classification problem, where each pair of sentences is labeled with one of two classes:\n",
        "\n",
        "- `0`: The sentences are not equivalent.\n",
        "- `1`: The sentences are equivalent.\n",
        "\n",
        "The model outputs logits, which are raw predictions that have not been normalized into probabilities. Each logit corresponds to one of the two classes. For each pair of sentences in your `test_dataset`, the model will output two numbers:\n",
        "\n",
        "- The first number corresponds to the model's confidence that the sentences are not equivalent (class `0`).\n",
        "- The second number corresponds to the model's confidence that the sentences are equivalent (class `1`).\n",
        "\n",
        "To get from these logits to an actual prediction, you would typically do the following:\n",
        "\n",
        "1. Apply the softmax function to the logits to convert them into probabilities. The softmax function will convert the raw logit scores into values between 0 and 1 that sum to 1, effectively giving you the probability of each class.\n",
        "   \n",
        "2. Take the argmax of the probabilities. This step involves choosing the index of the highest probability, which corresponds to the predicted class. If the first number is higher, the predicted class is `0` (not equivalent); if the second number is higher, the predicted class is `1` (equivalent)."
      ],
      "metadata": {
        "id": "EL1h45oiucyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "data = tfds.load('glue/mrpc')\n",
        "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
        "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
        "test_dataset = glue_convert_examples_to_features(data['test'], tokenizer, max_length=128, task='mrpc')\n",
        "test_dataset = test_dataset.shuffle(100).batch(32).repeat(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v2jWaNwoSOD",
        "outputId": "d92e4a52-1738-4096-f216-f5aa8ae3a262"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hMun_I6Gob3G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can then be compiled and trained as any Keras model:"
      ],
      "metadata": {
        "id": "40R1RYAepDh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train"
      ],
      "metadata": {
        "id": "OV6cXuBlszH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us walk through the code before we run it.\n",
        "\n",
        "The given code snippet is configuring and initiating the training process for the machine learning model (presumably the BERT model for sequence classification we discussed earlier) using TensorFlow's Keras API. Here's the breakdown of each line:\n",
        "\n",
        "1. `optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)`\n",
        "   - This line initializes an Adam optimizer, which is an algorithm for gradient-based optimization of stochastic objective functions.\n",
        "   - `tf.keras.optimizers.Adam` refers to the Adam optimizer class in TensorFlow's Keras API.\n",
        "   - `learning_rate=3e-5` sets the learning rate to `0.00003`. The learning rate is a hyperparameter that controls how much to adjust the model's parameters in response to the estimated error each time the model weights are updated.\n",
        "\n",
        "2. `loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n",
        "   - This line creates a loss function that the model will use to measure its performance.\n",
        "   - `tf.keras.losses.SparseCategoricalCrossentropy` is a loss function that is used when the labels are integers (as opposed to one-hot encoded vectors).\n",
        "   - `from_logits=True` indicates that the output values of the model are not normalized (e.g., with a softmax function), and the loss function will perform the normalization as part of its calculation.\n",
        "\n",
        "3. `model.compile(optimizer=optimizer, loss=loss)`\n",
        "   - This line configures the model for training by setting the optimizer and loss function.\n",
        "   - `model.compile` is a method to compile the model, preparing it for training by associating it with the specified optimizer and loss function.\n",
        "   - `optimizer=optimizer` sets the optimizer for the training process, and `loss=loss` sets the loss function to calculate the errors.\n",
        "\n",
        "4. `model.fit(train_dataset, epochs=2, steps_per_epoch=64)`\n",
        "   - This line starts training the model on the dataset that has been prepared.\n",
        "   - `model.fit` is the method to train the model for a fixed number of epochs (iterations over a dataset).\n",
        "   - `train_dataset` is the training dataset that the model will learn from.\n",
        "   - `epochs=2` tells the model to train for 2 complete passes over the training dataset.\n",
        "   - `steps_per_epoch=64` indicates the number of batch updates to perform before completing one epoch. Since an epoch is typically defined as one pass over the entire dataset, specifying `steps_per_epoch` is useful when the exact size of the dataset is not known or when using generators to produce data indefinitely.\n",
        "\n",
        "Together, these lines of code set up the optimizer and loss function, compile the model with these settings, and then train the model using the training dataset."
      ],
      "metadata": {
        "id": "IJTcy21KtPI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "model.fit(train_dataset, epochs=2, steps_per_epoch=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSyzEiQboS8e",
        "outputId": "81048f12-05e7-4b87-9d58-1bc89d5cc2f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "64/64 [==============================] - 93s 753ms/step - loss: 0.6003\n",
            "Epoch 2/2\n",
            "64/64 [==============================] - 50s 780ms/step - loss: 0.4966\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78d864239bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "tZdI0_Mtsxk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__oz-KKNsEzc",
        "outputId": "67b07c25-37a9-491f-c475-b5b65456c660"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108/108 [==============================] - 34s 286ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluaate Performance"
      ],
      "metadata": {
        "id": "Zir1rUpVssv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "VDD4HR3qss3e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax to logits to get probabilities.\n",
        "probabilities = tf.nn.softmax(yhat.logits, axis=-1)\n",
        "\n",
        "# Use argmax to get the predicted class index.\n",
        "predicted_class_indices = tf.argmax(probabilities, axis=-1)\n",
        "\n",
        "# Assuming you have two classes, map indices to class names.\n",
        "class_names = ['not equivalent', 'equivalent']\n",
        "predicted_classes = [class_names[index] for index in predicted_class_indices.numpy()]\n",
        "\n",
        "# Print the first 10 predictions.\n",
        "print(predicted_classes[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTLFKmBRsM8E",
        "outputId": "8ee2330f-55a4-4b58-c7af-2fca46888535"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['not equivalent', 'equivalent', 'equivalent', 'equivalent', 'equivalent', 'equivalent', 'not equivalent', 'equivalent', 'equivalent', 'equivalent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Models\n",
        "\n",
        "With the tight interoperability between TensorFlow and PyTorch models, you can even save the model and then reload it as a PyTorch model (or vice-versa):"
      ],
      "metadata": {
        "id": "LVSkxEuKpGMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model.save_pretrained('./my_mrpc_model/')\n",
        "pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llmvwhdhoVAo",
        "outputId": "2986c8d9-ed1e-48bb-8619-e66d4c59423d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer\n",
        "\n",
        "We also provide a simple but feature-complete training and evaluation interface through `Trainer()` and `TFTrainer()`. You can train, fine-tune, and evaluate any 🤗 Transformers model with a wide range of training options and with built-in features like logging, gradient accumulation, and mixed precision.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Z3Wtz70pJWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "jZ1uRkPAoXpR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEsPfncXpb-y",
        "outputId": "cb6c1e19-fe13-438e-cd24-bb3c5f046599"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the following error\n",
        "\n",
        "```\n",
        "ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
        "```"
      ],
      "metadata": {
        "id": "LNRwhckApij6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XvZ4gaMplpt",
        "outputId": "5be49ab0-a1ac-4ef8-819d-a84f87f642e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IuaKxxNpqZ8",
        "outputId": "b135e37e-5b1c-4e8e-f176-650efa79dfeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have to restart the runtime."
      ],
      "metadata": {
        "id": "XheGUUrdqLke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # tensorflow_datasets training dataset\n",
        "    eval_dataset=test_dataset            # tensorflow_datasets evaluation dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvv4_Pg5pzjX",
        "outputId": "352a4031-773b-4877-931b-8dbbdacf0ff1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now simply call `trainer.train()` to train and `trainer.evaluate()` to evaluate. You can use your own module as well, but the first argument returned from `forward` must be the loss which you wish to optimize.\n",
        "\n"
      ],
      "metadata": {
        "id": "sLcBOCu6pO_L"
      }
    }
  ]
}