{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX6vB-OppNfv"
      },
      "source": [
        "## LeNet\n",
        "\n",
        "LeNet is a classic convolutional neural network (CNN) architecture designed by Yann LeCun, which was primarily used for handwritten digit recognition. Here's a simple implementation of the LeNet model using TensorFlow 2.x and Keras:\n",
        "\n",
        "```py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_lenet_model(input_shape=(32, 32, 1), num_classes=10):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Layer 1: Convolutional Layer + Activation + Pooling\n",
        "    model.add(layers.Conv2D(filters=6, kernel_size=(5, 5), padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "\n",
        "    # Layer 2: Convolutional Layer + Activation + Pooling\n",
        "    model.add(layers.Conv2D(filters=16, kernel_size=(5, 5), padding='valid', activation='relu'))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "\n",
        "    # Layer 3: Convolutional Layer + Activation\n",
        "    model.add(layers.Conv2D(filters=120, kernel_size=(5, 5), padding='valid', activation='relu'))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Layer 4: Fully Connected (Dense) Layer + Activation\n",
        "    model.add(layers.Dense(units=84, activation='relu'))\n",
        "\n",
        "    # Layer 5: Fully Connected (Dense) Layer + Activation (Output Layer)\n",
        "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the LeNet model\n",
        "lenet_model = build_lenet_model(input_shape=(32, 32, 1), num_classes=10)\n",
        "lenet_model.summary()\n",
        "```\n",
        "\n",
        "LeNet is a convolutional neural network, and its original paper is called \"Gradient-Based Learning Applied to Document Recognition\" by Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. It was published in 1998 and can be found here: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kX1-wPypS8R"
      },
      "source": [
        "## VGG16\n",
        "\n",
        "VGG16 is a deep convolutional neural network (CNN) architecture developed by the Visual Geometry Group at the University of Oxford. Here's a simple implementation of the VGG16 model using TensorFlow 2.x and Keras Sequential API:\n",
        "\n",
        "```py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_vgg16_model(input_shape=(224, 224, 3), num_classes=1000):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1: 2x Convolutional Layers + Max Pooling\n",
        "    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 2: 2x Convolutional Layers + Max Pooling\n",
        "    model.add(layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 3: 3x Convolutional Layers + Max Pooling\n",
        "    model.add(layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 4: 3x Convolutional Layers + Max Pooling\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 5: 3x Convolutional Layers + Max Pooling\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the VGG16 model\n",
        "vgg16_model = build_vgg16_model(input_shape=(224, 224, 3), num_classes=1000)\n",
        "vgg16_model.summary\n",
        "```\n",
        "\n",
        "VGG16 is introduced in a paper titled \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" by K. Simonyan and A. Zisserman.\n",
        "\n",
        "Here is the link to the VGG16 paper: \n",
        "\n",
        "[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6md-gIspYG-"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "ResNet (Residual Network) is a deep convolutional neural network (CNN) architecture that introduces residual connections to improve learning and avoid the vanishing gradient problem in deep networks. Here's a simple implementation of the ResNet-50 model using TensorFlow 2.x and Keras functional API:\n",
        "\n",
        "```py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "def identity_block(input_tensor, kernel_size, filters):\n",
        "    filters1, filters2, filters3 = filters\n",
        "\n",
        "    x = layers.Conv2D(filters1, (1, 1))(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, (1, 1))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
        "    filters1, filters2, filters3 = filters\n",
        "\n",
        "    x = layers.Conv2D(filters1, (1, 1), strides=strides)(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, (1, 1))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides)(input_tensor)\n",
        "    shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def build_resnet50(input_shape=(224, 224, 3), num_classes=1000):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolution layer\n",
        "    x = layers.ZeroPadding2D(padding=(3, 3))(input_tensor)\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256])\n",
        "    x = identity_block(x, 3, [64, 64, 256])\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512])\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "\n",
        "    x = conv_block(x, 3, [256, 256, 1024])\n",
        "    x = identity_block(x, 3, [256, 256, 1024])\n",
        "    x = identity_block(x, 3, [256, \n",
        "```\n",
        "\n",
        "ResNet was introduced in the paper \"Deep Residual Learning for Image Recognition\" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Here's the link to the ResNet paper:\n",
        "\n",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIMwTxKhpcCs"
      },
      "source": [
        "## DenseNet\n",
        "\n",
        "DenseNet (Densely Connected Convolutional Networks) is a deep convolutional neural network (CNN) architecture that introduces dense connections between layers to improve learning and parameter efficiency. Here's a simple implementation of the DenseNet-121 model using TensorFlow 2.x and Keras functional API:\n",
        "\n",
        "```py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "def dense_block(x, num_layers, growth_rate):\n",
        "    for _ in range(num_layers):\n",
        "        output = layers.BatchNormalization()(x)\n",
        "        output = layers.Activation('relu')(output)\n",
        "        output = layers.Conv2D(4 * growth_rate, (1, 1), padding='same', kernel_initializer='he_normal')(output)\n",
        "        output = layers.BatchNormalization()(output)\n",
        "        output = layers.Activation('relu')(output)\n",
        "        output = layers.Conv2D(growth_rate, (3, 3), padding='same', kernel_initializer='he_normal')(output)\n",
        "        x = layers.Concatenate()([x, output])\n",
        "    return x\n",
        "\n",
        "def transition_layer(x, compression_factor):\n",
        "    num_filters = int(x.shape[-1] * compression_factor)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(num_filters, (1, 1), padding='same', kernel_initializer='he_normal')(x)\n",
        "    x = layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def build_densenet121(input_shape=(224, 224, 3), num_classes=1000, growth_rate=32, compression_factor=0.5):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolution layer\n",
        "    x = layers.Conv2D(2 * growth_rate, (7, 7), strides=(2, 2), padding='same', kernel_initializer='he_normal')(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Dense blocks and transition layers\n",
        "    x = dense_block(x, 6, growth_rate)\n",
        "    x = transition_layer(x, compression_factor)\n",
        "\n",
        "    x = dense_block(x, 12, growth_rate)\n",
        "    x = transition_layer(x, compression_factor)\n",
        "\n",
        "    x = dense_block(x, 24, growth_rate)\n",
        "    x = transition_layer(x, compression_factor)\n",
        "\n",
        "    x = dense_block(x, 16, growth_rate)\n",
        "\n",
        "    # Global average pooling and output layer\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(input_tensor, x, name='densenet121')\n",
        "    return model\n",
        "\n",
        "# Build the DenseNet-121 model\n",
        "densenet121_model = build_densenet121(input_shape=(224, 224, 3), num_classes=1000)\n",
        "densenet121_model.summary()\n",
        "```\n",
        "\n",
        "DenseNet was introduced in the paper \"Densely Connected Convolutional Networks\" by Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Here is the link to the original paper for Densenet: \n",
        "\n",
        "https://arxiv.org/abs/1608.06993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9omz0AL9qe3p"
      },
      "source": [
        "## Inception\n",
        "\n",
        "Inception is a deep convolutional neural network (CNN) architecture that was introduced in the GoogLeNet model. It uses inception modules to efficiently learn different features at multiple scales. Here's a simple implementation of the Inception V1 (GoogLeNet) model using TensorFlow 2.x and Keras functional API:\n",
        "\n",
        "```py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj):\n",
        "    conv_1x1 = layers.Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "    \n",
        "    conv_3x3_reduce = layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "    conv_3x3 = layers.Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal')(conv_3x3_reduce)\n",
        "    \n",
        "    conv_5x5_reduce = layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "    conv_5x5 = layers.Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer='he_normal')(conv_5x5_reduce)\n",
        "    \n",
        "    max_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = layers.Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(max_pool)\n",
        "    \n",
        "    output = layers.Concatenate(axis=-1)([conv_1x1, conv_3x3, conv_5x5, pool_proj])\n",
        "    return output\n",
        "\n",
        "def build_inception_v1(input_shape=(224, 224, 3), num_classes=1000):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolution and max pooling layers\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Local response normalization\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Convolution and max pooling layers\n",
        "    x = layers.Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "    x = layers.Conv2D(192, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "\n",
        "    # Local response normalization\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Inception modules\n",
        "    x = inception_module(x, filters_1x1=64, filters_3x3_reduce=96, filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool_proj=32)\n",
        "    x = inception_module(x, filters_1x1=128, filters_\n",
        "    x = inception_module(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool_proj=64)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = inception_module(x, filters_1x1=192, filters_3x3_reduce=96, filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool_proj=64)\n",
        "    x = inception_module(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
        "    x = inception_module(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n",
        "    x = inception_module(x, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32, filters_5x5=64, filters_pool_proj=64)\n",
        "    x = inception_module(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = inception_module(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n",
        "    x = inception_module(x, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool_proj=128)\n",
        "\n",
        "    # Global average pooling and output layer\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(input_tensor, x, name='inception_v1')\n",
        "    return model\n",
        "\n",
        "# Build the Inception V1 (GoogLeNet) model\n",
        "inception_v1_model = build_inception_v1(input_shape=(224, 224, 3), num_classes=1000)\n",
        "inception_v1_model.summary()\n",
        "```\n",
        "\n",
        "The paper that introduced the Inception model is titled \"Going deeper with convolutions\" by Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. The paper for Inception model can be found at this link: https://arxiv.org/abs/1409.4842\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJw0Ih5DpJSi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
