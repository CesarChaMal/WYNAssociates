{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Get Packages"
      ],
      "metadata": {
        "id": "CMIG2JzfrJH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The commands you've written are for interacting with Git and Pip, which are commonly used tools for software development. Here's a breakdown of what they do:\n",
        "\n",
        "1. `! git clone https://github.com/yiqiao-yin/wyn-chatbot-io.git`:\n",
        "   - `!` is often used in the context of a Jupyter Notebook to execute shell commands.\n",
        "   - `git clone` is a Git command used to clone (or copy) a repository from an existing URL to your local machine.\n",
        "   - `https://github.com/yiqiao-yin/wyn-chatbot-io.git` is the URL of the repository you're cloning. In this case, it's the \"wyn-chatbot-io\" repository from the GitHub user \"yiqiao-yin\".\n",
        "\n",
        "2. `! pip install -r /content/wyn-chatbot-io/requirements.txt`:\n",
        "   - `!` again is used to execute shell commands in a Jupyter Notebook.\n",
        "   - `pip install` is a command to install Python packages.\n",
        "   - `-r` is an option for `pip install` which allows you to install multiple packages listed in a file.\n",
        "   - `/content/wyn-chatbot-io/requirements.txt` is the path to the requirements file in the cloned repository. This file typically contains a list of packages and their versions that are necessary for the project in the repository to run.\n",
        "\n",
        "After running these commands, you would have cloned the repository into your local environment and installed the necessary Python packages listed in the `requirements.txt` file."
      ],
      "metadata": {
        "id": "Q5ckDEsLrLCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/yiqiao-yin/wyn-chatbot-io.git"
      ],
      "metadata": {
        "id": "jWptsM4k4fwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/wyn-chatbot-io/requirements.txt"
      ],
      "metadata": {
        "id": "wBqZ8Aw94m-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cFN0eJcL25A_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/new.csv')"
      ],
      "metadata": {
        "id": "j_j3rG6S3Eu7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "df['questions'] = df['question']"
      ],
      "metadata": {
        "id": "Fwg5CVVU3J1F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "nUZWzR0k3KWv",
        "outputId": "a91b82a5-fbeb-4ccc-ae77-0c2ff66c5011"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context  \\\n",
              "0  Heliconia : Heliconias are a genus of flowerin...   \n",
              "1  cultures across the Amazon use heliconia leave...   \n",
              "2  leaves and cook them over fire or in water. So...   \n",
              "3  bananas, but tend to be less favorable because...   \n",
              "4  species. The flowers of the heliconias create ...   \n",
              "\n",
              "                                            question  \\\n",
              "0             What is the native range of Heliconia?   \n",
              "1   What is the dish called and what is wrapped i...   \n",
              "2   What are the similarities between heliconia f...   \n",
              "3   What is the difference between a banana and a...   \n",
              "4   What is the significance of the flowers of th...   \n",
              "\n",
              "                                             answers  \\\n",
              "0   The native range of Heliconia is global, with...   \n",
              "1   The dish is called maito and the foods that a...   \n",
              "2  \\n\\nThe similarities between heliconia fruits ...   \n",
              "3   Bananas are typically sweeter and have fewer ...   \n",
              "4  \\n\\nThe flowers of the heliconias create nurse...   \n",
              "\n",
              "                                           questions  \n",
              "0             What is the native range of Heliconia?  \n",
              "1   What is the dish called and what is wrapped i...  \n",
              "2   What are the similarities between heliconia f...  \n",
              "3   What is the difference between a banana and a...  \n",
              "4   What is the significance of the flowers of th...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-5292b548-3c23-479a-b95e-22bbd2aea4b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "      <th>questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heliconia : Heliconias are a genus of flowerin...</td>\n",
              "      <td>What is the native range of Heliconia?</td>\n",
              "      <td>The native range of Heliconia is global, with...</td>\n",
              "      <td>What is the native range of Heliconia?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cultures across the Amazon use heliconia leave...</td>\n",
              "      <td>What is the dish called and what is wrapped i...</td>\n",
              "      <td>The dish is called maito and the foods that a...</td>\n",
              "      <td>What is the dish called and what is wrapped i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>leaves and cook them over fire or in water. So...</td>\n",
              "      <td>What are the similarities between heliconia f...</td>\n",
              "      <td>\\n\\nThe similarities between heliconia fruits ...</td>\n",
              "      <td>What are the similarities between heliconia f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bananas, but tend to be less favorable because...</td>\n",
              "      <td>What is the difference between a banana and a...</td>\n",
              "      <td>Bananas are typically sweeter and have fewer ...</td>\n",
              "      <td>What is the difference between a banana and a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>species. The flowers of the heliconias create ...</td>\n",
              "      <td>What is the significance of the flowers of th...</td>\n",
              "      <td>\\n\\nThe flowers of the heliconias create nurse...</td>\n",
              "      <td>What is the significance of the flowers of th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5292b548-3c23-479a-b95e-22bbd2aea4b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-6f01174f-2f6e-4628-b343-b5613f97320e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f01174f-2f6e-4628-b343-b5613f97320e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-6f01174f-2f6e-4628-b343-b5613f97320e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5292b548-3c23-479a-b95e-22bbd2aea4b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5292b548-3c23-479a-b95e-22bbd2aea4b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduce Functions\n",
        "\n",
        "Here's a summary of the functions:\n",
        "\n",
        "1. **calculate_cosine_similarity**\n",
        "   - Takes two input sentences.\n",
        "   - Tokenizes the sentences into lowercase words.\n",
        "   - Creates a set of unique words from both sentences.\n",
        "   - Creates a frequency vector for each sentence based on the unique words.\n",
        "   - Calculates the cosine similarity between the frequency vectors.\n",
        "   - Returns the cosine similarity as a float between 0 and 1.\n",
        "\n",
        "2. **calculate_sts_score**\n",
        "   - Takes two input sentences.\n",
        "   - Loads a pre-trained SentenceTransformer model (\"paraphrase-MiniLM-L6-v2\").\n",
        "   - Computes sentence embeddings for each input sentence.\n",
        "   - Calculates cosine similarity between the embeddings.\n",
        "   - Returns the similarity score as a float.\n",
        "\n",
        "3. **openai_text_embedding**\n",
        "   - Takes a text prompt as input.\n",
        "   - Returns the text embedding generated by OpenAI's API with model \"text-embedding-ada-002\".\n",
        "\n",
        "4. **calculate_sts_openai_score**\n",
        "   - Takes two input sentences.\n",
        "   - Computes sentence embeddings using the `openai_text_embedding` function.\n",
        "   - Converts the embeddings to arrays.\n",
        "   - Calculates cosine similarity between the embeddings.\n",
        "   - Returns the similarity score as a float.\n",
        "\n",
        "5. **palm_text_embedding**\n",
        "   - Takes a text prompt as input.\n",
        "   - Returns the text embedding generated by the `palm` function with model \"embedding-gecko-001\".\n",
        "\n",
        "6. **calculate_sts_palm_score**\n",
        "   - Takes two input sentences.\n",
        "   - Computes sentence embeddings using the `palm_text_embedding` function.\n",
        "   - Converts the embeddings to arrays.\n",
        "   - Calculates cosine similarity between the embeddings.\n",
        "   - Returns the similarity score as a float.\n",
        "\n",
        "Note: In all functions, the cosine similarity is calculated by subtracting the cosine distance from 1. Also, the term \"embedding\" typically refers to a numerical representation of text that captures semantic information. The exact nature of these embeddings depends on the model used to create them."
      ],
      "metadata": {
        "id": "n5pYSWu0rTEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Union\n",
        "import google.generativeai as palm\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "7dZJtm-U4Uu1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(sentence1: str, sentence2: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity between two sentences.\n",
        "\n",
        "    Args:\n",
        "        sentence1 (str): The first sentence.\n",
        "        sentence2 (str): The second sentence.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity between the two sentences, represented as a float value between 0 and 1.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentences into words\n",
        "    words1 = sentence1.lower().split()\n",
        "    words2 = sentence2.lower().split()\n",
        "\n",
        "    # Create a set of unique words from both sentences\n",
        "    unique_words = set(words1 + words2)\n",
        "\n",
        "    # Create a frequency vector for each sentence\n",
        "    freq_vector1 = np.array([words1.count(word) for word in unique_words])\n",
        "    freq_vector2 = np.array([words2.count(word) for word in unique_words])\n",
        "\n",
        "    # Calculate the cosine similarity between the frequency vectors\n",
        "    similarity = 1 - cosine(freq_vector1, freq_vector2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def calculate_sts_score(sentence1: str, sentence2: str) -> float:\n",
        "    model = SentenceTransformer(\n",
        "        \"paraphrase-MiniLM-L6-v2\"\n",
        "    )  # Load a pre-trained STS model\n",
        "\n",
        "    # Compute sentence embeddings\n",
        "    embedding1 = model.encode([sentence1])[0]  # Flatten the embedding array\n",
        "    embedding2 = model.encode([sentence2])[0]  # Flatten the embedding array\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = 1 - cosine(embedding1, embedding2)\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def openai_text_embedding(prompt: str) -> str:\n",
        "    return openai.Embedding.create(input=prompt, model=\"text-embedding-ada-002\")[\n",
        "        \"data\"\n",
        "    ][0][\"embedding\"]\n",
        "\n",
        "\n",
        "def calculate_sts_openai_score(sentence1: str, sentence2: str) -> float:\n",
        "    # Compute sentence embeddings\n",
        "    embedding1 = openai_text_embedding(sentence1)  # Flatten the embedding array\n",
        "    embedding2 = openai_text_embedding(sentence2)  # Flatten the embedding array\n",
        "\n",
        "    # Convert to array\n",
        "    embedding1 = np.asarray(embedding1)\n",
        "    embedding2 = np.asarray(embedding2)\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = 1 - cosine(embedding1, embedding2)\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def palm_text_embedding(prompt: str) -> str:\n",
        "    model = \"models/embedding-gecko-001\"\n",
        "    return palm.generate_embeddings(model=model, text=prompt)[\"embedding\"]\n",
        "\n",
        "\n",
        "def calculate_sts_palm_score(sentence1: str, sentence2: str) -> float:\n",
        "    # Compute sentence embeddings\n",
        "    embedding1 = palm_text_embedding(sentence1)  # Flatten the embedding array\n",
        "    embedding2 = palm_text_embedding(sentence2)  # Flatten the embedding array\n",
        "\n",
        "    # Convert to array\n",
        "    embedding1 = np.asarray(embedding1)\n",
        "    embedding2 = np.asarray(embedding2)\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity_score = 1 - cosine(embedding1, embedding2)\n",
        "\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "hCiyvI6f3_LU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `add_dist_score_column`, takes a dataframe and a sentence as input arguments. It calculates similarity scores between the input sentence and a column of sentences (\"questions\") in the dataframe, based on a specified similarity measure. It then adds a column of these similarity scores to the dataframe, sorts the dataframe by the similarity scores in descending order, and returns the top five rows of the sorted dataframe.\n",
        "\n",
        "Here are the steps the function performs:\n",
        "\n",
        "1. **Input Parameters:**\n",
        "   - `dataframe`: a Pandas DataFrame that should have a column named \"questions\" containing sentences to be compared against the input sentence.\n",
        "   - `sentence`: a single sentence (string) that will be compared against each sentence in the \"questions\" column of the dataframe.\n",
        "   - `similarity_indicator`: a string that specifies the similarity measure to be used. It can be one of \"cosine\", \"levenshtein\", \"sts\", \"stsopenai\", or \"stspalm\". If an unsupported value is passed, it defaults to \"cosine\".\n",
        "\n",
        "2. **Calculate Similarity Scores and Add to DataFrame:**\n",
        "   - If `similarity_indicator` is \"cosine\", the function calculates cosine similarity scores between the input sentence and each sentence in the \"questions\" column using the `calculate_cosine_similarity` function. It then adds a column named \"cosine\" to the dataframe containing these scores.\n",
        "   - If `similarity_indicator` is \"levenshtein\", it calculates cosine similarity scores (though it should probably be calculating Levenshtein distances) and adds a column named \"levenshtein\".\n",
        "   - If `similarity_indicator` is \"sts\", it calculates similarity scores using the `calculate_sts_score` function and adds a column named \"sts\".\n",
        "   - If `similarity_indicator` is \"stsopenai\", it calculates similarity scores using the `calculate_sts_openai_score` function and adds a column named \"stsopenai\".\n",
        "   - If `similarity_indicator` is \"stspalm\", it calculates similarity scores using the `calculate_sts_palm_score` function and adds a column named \"stspalm\".\n",
        "   - If an unsupported value is passed for `similarity_indicator`, it defaults to calculating cosine similarity scores and adding a column named \"cosine\".\n",
        "\n",
        "3. **Sort DataFrame and Return Top Rows:**\n",
        "   - The function sorts the dataframe by the similarity scores in descending order (highest scores at the top).\n",
        "   - It then returns the top five rows of the sorted dataframe.\n",
        "\n",
        "Note: The function appears to have an error in the \"levenshtein\" case where it calculates cosine similarity scores instead of Levenshtein distances. If Levenshtein distances were intended to be used, a separate function for calculating those distances should be called instead."
      ],
      "metadata": {
        "id": "Tsveu1UBu4rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dist_score_column(\n",
        "    dataframe: pd.DataFrame, sentence: str, similarity_indicator: str = \"cosine\"\n",
        ") -> pd.DataFrame:\n",
        "    if similarity_indicator == \"cosine\":\n",
        "        dataframe[\"cosine\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_cosine_similarity(x, sentence)\n",
        "        )\n",
        "    elif similarity_indicator == \"levenshtein\":\n",
        "        dataframe[\"levenshtein\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_cosine_similarity(x, sentence)\n",
        "        )\n",
        "    elif similarity_indicator == \"sts\":\n",
        "        dataframe[\"sts\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_sts_score(x, sentence)\n",
        "        )\n",
        "    elif similarity_indicator == \"stsopenai\":\n",
        "        dataframe[\"stsopenai\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_sts_openai_score(str(x), sentence)\n",
        "        )\n",
        "    elif similarity_indicator == \"stspalm\":\n",
        "        dataframe[\"stspalm\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_sts_palm_score(str(x), sentence)\n",
        "        )\n",
        "    else:\n",
        "        dataframe[\"cosine\"] = dataframe[\"questions\"].apply(\n",
        "            lambda x: calculate_cosine_similarity(x, sentence)\n",
        "        )\n",
        "\n",
        "    sorted_dataframe = dataframe.sort_values(by=similarity_indicator, ascending=False)\n",
        "\n",
        "    return sorted_dataframe.iloc[:5, :]\n"
      ],
      "metadata": {
        "id": "B10G3Bdk31DQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function called `convert_to_list_of_dict`, which takes a pandas DataFrame as input and converts it into a list of dictionaries. Each dictionary represents a question or answer and has two keys: \"role\" and \"content.\"\n",
        "\n",
        "Here is a step-by-step explanation of what the code does:\n",
        "\n",
        "1. **Function Definition:**\n",
        "   - The function is defined with the name `convert_to_list_of_dict` and takes one argument `df`, which is expected to be a pandas DataFrame.\n",
        "   - The function is expected to return a list of dictionaries, with each dictionary having a string key-value pair.\n",
        "\n",
        "2. **Function Documentation:**\n",
        "   - The function is documented with a docstring that explains its purpose, input parameters, and return value.\n",
        "   - It reads in a pandas DataFrame with columns named 'questions' and 'answers' and produces a list of dictionaries with two keys each: 'question' and 'answer.'\n",
        "\n",
        "3. **Initialize an Empty List:**\n",
        "   - The function initializes an empty list called `result`, which will be used to store the dictionaries created from the DataFrame rows.\n",
        "\n",
        "4. **Loop through the DataFrame Rows:**\n",
        "   - The function iterates through each row of the input DataFrame using the `iterrows()` method.\n",
        "   - For each row, it retrieves the values in the \"questions\" and \"answers\" columns.\n",
        "\n",
        "5. **Create Question and Answer Dictionaries:**\n",
        "   - For each row, the function creates two dictionaries:\n",
        "     - `qa_dict_quest`: This dictionary represents a question and has two key-value pairs: \"role\" is set to \"user\" and \"content\" is set to the value in the \"questions\" column of the current row.\n",
        "     - `qa_dict_ans`: This dictionary represents an answer and has two key-value pairs: \"role\" is set to \"assistant\" and \"content\" is set to the value in the \"answers\" column of the current row.\n",
        "\n",
        "6. **Add Dictionaries to the Result List:**\n",
        "   - The function adds the two dictionaries created in the previous step to the `result` list. It first adds the question dictionary and then the answer dictionary.\n",
        "\n",
        "7. **Return the Result List:**\n",
        "   - After iterating through all the rows of the DataFrame and adding the corresponding dictionaries to the `result` list, the function returns the `result` list.\n",
        "\n",
        "The returned list can be used to represent a sequence of questions and answers in a chatbot conversation, where each dictionary represents a message in the conversation with information about the sender's role (user or assistant) and the content of the message."
      ],
      "metadata": {
        "id": "28s5UJ-dvGrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list_of_dict(df: pd.DataFrame) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Reads in a pandas DataFrame and produces a list of dictionaries with two keys each, 'question' and 'answer.'\n",
        "\n",
        "    Args:\n",
        "        df: A pandas DataFrame with columns named 'questions' and 'answers'.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, with each dictionary containing a 'question' and 'answer' key-value pair.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the dictionaries\n",
        "    result = []\n",
        "\n",
        "    # Loop through each row of the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Create a dictionary with the current question and answer\n",
        "        qa_dict_quest = {\"role\": \"user\", \"content\": row[\"questions\"]}\n",
        "        qa_dict_ans = {\"role\": \"assistant\", \"content\": row[\"answers\"]}\n",
        "\n",
        "        # Add the dictionary to the result list\n",
        "        result.append(qa_dict_quest)\n",
        "        result.append(qa_dict_ans)\n",
        "\n",
        "    # Return the list of dictionaries\n",
        "    return result"
      ],
      "metadata": {
        "id": "OJgpUnzp38ic"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter API Key"
      ],
      "metadata": {
        "id": "-a83JZDtxgUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"ENTER API KEY HERE\""
      ],
      "metadata": {
        "id": "fxec4vgs7Baj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key = \"ENTER API KEY HERE\")"
      ],
      "metadata": {
        "id": "DHhU6Zr_7Hn8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_chatgpt(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses the OpenAI API to generate an AI response to a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: A string representing the prompt to send to the OpenAI API.\n",
        "\n",
        "    Returns:\n",
        "        A string representing the AI's generated response.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the OpenAI API to generate a response based on the input prompt.\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=800,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    # Extract the text from the first (and only) choice in the response output.\n",
        "    ans = response.choices[0][\"text\"]\n",
        "\n",
        "    # Return the generated AI response.\n",
        "    return ans"
      ],
      "metadata": {
        "id": "TZ0gBIVR65In"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_palm(prompt: str) -> str:\n",
        "    completion = palm.generate_text(\n",
        "        model=\"models/text-bison-001\",\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_output_tokens=800,\n",
        "    )\n",
        "\n",
        "    return completion.result"
      ],
      "metadata": {
        "id": "EN_-X4BU68Mr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"hello\"\n",
        "similarity_indicator = \"cosine\"\n",
        "\n",
        "def get_ans_from_no_reference(user_input: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses the LLM to generate an AI response to a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: A string representing the prompt to send to the LLM.\n",
        "\n",
        "    Returns:\n",
        "        A string representing the AI's generated response.\n",
        "    \"\"\"\n",
        "    output = call_chatgpt(user_input)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_answers_from_gpt(user_input: str, similarity_indicator) -> str:\n",
        "    \"\"\"\n",
        "    Uses the LLM to generate an AI response to a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: A string representing the prompt to send to the LLM.\n",
        "\n",
        "    Returns:\n",
        "        A string representing the AI's generated response.\n",
        "    \"\"\"\n",
        "\n",
        "    df_screened_by_dist_score = add_dist_score_column(\n",
        "        df, user_input, similarity_indicator\n",
        "    )\n",
        "    qa_pairs = convert_to_list_of_dict(df_screened_by_dist_score)\n",
        "    processed_user_question = f\"\"\"\n",
        "        Learn from the context: {qa_pairs}\n",
        "        Answer the following question as if you are the AI assistant: {user_input}\n",
        "        Produce a text answer that are complete sentences.\n",
        "    \"\"\"\n",
        "    output = call_chatgpt(processed_user_question)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "aE5jWJHD3WyS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Experiments"
      ],
      "metadata": {
        "id": "aXaDI4dvxjFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use `.apply` to Generate Answers"
      ],
      "metadata": {
        "id": "AGBDcF7MxGg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is using a Python cell magic command, `%%time`, which is a feature of Jupyter notebooks. It is used to time the execution of the code in the cell.\n",
        "\n",
        "The code snippet applies a function, `get_ans_from_no_reference`, to each row of the \"question\" column in the dataframe `df`. It does this by using the `apply` method of the Pandas DataFrame. The result is then stored in a new column of the dataframe, \"ans_from_gpt_no_ref\".\n",
        "\n",
        "Here is the step-by-step breakdown of the code:\n",
        "\n",
        "1. **Time the Execution:**\n",
        "   - The `%%time` magic command is used to measure the time it takes to execute the code in the cell.\n",
        "\n",
        "2. **Apply Function to DataFrame:**\n",
        "   - The `apply` method is called on the \"question\" column of the dataframe `df`.\n",
        "   - For each row in the \"question\" column, the function `get_ans_from_no_reference` is applied.\n",
        "   - The function is applied using a lambda function, which calls `get_ans_from_no_reference` with the current question (`x`) as the argument.\n",
        "\n",
        "3. **Store Results in New Column:**\n",
        "   - The results of applying the function to each row of the \"question\" column are stored in a new column of the dataframe, \"ans_from_gpt_no_ref\".\n",
        "\n",
        "The function `get_ans_from_no_reference` is defined as follows:\n",
        "\n",
        "1. **Function Definition:**\n",
        "   - The function is named `get_ans_from_no_reference` and takes one argument, `user_input`, which is a string.\n",
        "\n",
        "2. **Function Documentation:**\n",
        "   - The function is documented with a docstring that explains its purpose, input parameters, and return value.\n",
        "   - The function uses the LLM (presumably some kind of language model) to generate an AI response to a prompt.\n",
        "\n",
        "3. **Call External Function:**\n",
        "   - The function calls another function, `call_chatgpt`, with `user_input` as the argument.\n",
        "   - The result of the `call_chatgpt` function call is stored in the variable `output`.\n",
        "\n",
        "4. **Return the Output:**\n",
        "   - The function returns the value stored in the variable `output`.\n",
        "\n",
        "The purpose of this code is to generate AI responses to each question in the dataframe and store the responses in a new column. The time taken to perform this operation is displayed in the Jupyter notebook cell output."
      ],
      "metadata": {
        "id": "Vfvkz8DmxEZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['ans_from_gpt_no_ref'] = df.question.apply(lambda x: get_ans_from_no_reference(user_input=x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzJP_r3rpEx5",
        "outputId": "9442a8c1-ec1d-406b-c02e-0079d79fd37a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 906 ms, sys: 84.7 ms, total: 991 ms\n",
            "Wall time: 1min 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['ans_from_cosine+gpt'] = df.question.apply(lambda x: get_answers_from_gpt(user_input=x, similarity_indicator=\"cosine\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np-kQgSM6oHs",
        "outputId": "94605e26-1f04-42e7-e6d0-a2b858384728"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.61 s, sys: 99.9 ms, total: 1.71 s\n",
            "Wall time: 2min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['ans_from_stsopenai+gpt'] = df.question.apply(lambda x: get_answers_from_gpt(user_input=x, similarity_indicator=\"stsopenai\"))"
      ],
      "metadata": {
        "id": "XVhDQuua87jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['ans_from_stspalm+gpt'] = df.question.apply(lambda x: get_answers_from_gpt(user_input=x, similarity_indicator=\"stspalm\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99QDz7ph9Ghn",
        "outputId": "70dfb94a-6a6a-41c1-8983-2ffabf108333"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min, sys: 9.16 s, total: 2min 9s\n",
            "Wall time: 2h 2min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"answers\", \"ans_from_cosine+gpt\"]].iloc[0, :].apply(lambda x: x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTjeR-pelzix",
        "outputId": "5bbf4d1a-97e9-4108-85b6-892eba950033"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "answers                  \n",
              "ans_from_cosine+gpt    \\n\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df[\"ans_from_cosine+gpt_palmscore\"] = df[[\"answers\", \"ans_from_cosine+gpt\"]].apply(\n",
        "    lambda x: calculate_sts_palm_score(\n",
        "        x[0],\n",
        "        x[1]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYJlcOKtlELq",
        "outputId": "acdb92aa-bae1-466d-faa8-e78c8d149b51"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 58.8 ms, sys: 2.25 ms, total: 61.1 ms\n",
            "Wall time: 2.91 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['context', 'question', 'answers', 'questions', 'cosine',\n",
        "       'ans_from_cosine__gpt', 'stsopenai', 'stspalm', 'ans_from_stspalm__gpt',\n",
        "       'ans_from_cosine__gpt_palmscore', 'ans_from_gpt_no_ref']"
      ],
      "metadata": {
        "id": "40Vfm2_Tp1hA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Performance\n",
        "\n",
        "Use different options: 1) embedding, 2) LLM, 3) similarity scores, to evaluate the a) answers without reference, and b) answers with references.\n",
        "\n",
        "The goal is to show that we can produce better and more informative answers in the chatbot."
      ],
      "metadata": {
        "id": "w9hXIQPGxMDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "XlF6kIlBoG-Z"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_0 = []\n",
        "for i in tqdm(range(len(df))):\n",
        "    truth = df.answers[i]\n",
        "    prediction = df.ans_from_gpt_no_ref[i]\n",
        "    scores_0.append(calculate_sts_palm_score(truth, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKaVIvfNpck-",
        "outputId": "e1de1f0c-9cf7-458b-d6bc-1f7dbbb2c614"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83/83 [01:27<00:00,  1.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ans_from_gpt_no_ref_palmscore\"] = scores_0"
      ],
      "metadata": {
        "id": "k5ZcebIHpkz4"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "    The average similarity score based on the following premises:\n",
        "    1) no context or any reference,\n",
        "    2) just throw raw question into chatgpt,\n",
        "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
        "    is: {df[\"ans_from_gpt_no_ref_palmscore\"].mean()}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rlq1orZpqHu",
        "outputId": "75a987d8-5012-46e0-928d-cf7a399bf2e7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    The average similarity score based on the following premises:\n",
            "    1) no context or any reference, \n",
            "    2) just throw raw question into chatgpt,\n",
            "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
            "    is: 0.754197628682477\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for i in tqdm(range(len(df))):\n",
        "    x = df[[\"answers\", \"ans_from_cosine__gpt\"]].iloc[i, :]\n",
        "    scores.append(calculate_sts_palm_score(x[\"answers\"], x[\"ans_from_cosine__gpt\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDTEP4XImkfL",
        "outputId": "d76ec4ec-dca8-4455-aa51-1ff9ec32357c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83/83 [01:27<00:00,  1.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ans_from_cosine+gpt_palmscore\"] = scores"
      ],
      "metadata": {
        "id": "-IG_LVGxmY-h"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "    The average similarity score based on the following premises:\n",
        "    1) cosine as embedding,\n",
        "    2) chatgpt as llm,\n",
        "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
        "    is: {df[\"ans_from_cosine+gpt_palmscore\"].mean()}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXUUaX9toi4L",
        "outputId": "7743c971-bf5a-478e-9f75-01ba227ca9d0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    The average similarity score based on the following premises:\n",
            "    1) cosine as embedding, \n",
            "    2) chatgpt as llm,\n",
            "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
            "    is: 0.9182174069649235\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_1 = []\n",
        "for i in tqdm(range(len(df))):\n",
        "    x = df[[\"answers\", \"ans_from_stspalm__gpt\"]].iloc[i, :]\n",
        "    scores_1.append(calculate_sts_palm_score(x[\"answers\"], x[\"ans_from_stspalm__gpt\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlVUEFzOuoWy",
        "outputId": "7a916934-4a2e-4a8e-cdea-70e10d600acd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83/83 [01:28<00:00,  1.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ans_from_stspalm__gpt_palmscore\"] = scores_1"
      ],
      "metadata": {
        "id": "4Ops9JSJvaHp"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "    The average similarity score based on the following premises:\n",
        "    1) cosine as embedding,\n",
        "    2) palm as llm,\n",
        "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
        "    is: {df[\"ans_from_stspalm__gpt_palmscore\"].mean()}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTCUrZj8vkN0",
        "outputId": "2fd42698-1689-4eb8-e7c1-f04085c49111"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    The average similarity score based on the following premises:\n",
            "    1) cosine as embedding, \n",
            "    2) palm as llm,\n",
            "    3) and palm embedding to compute the similarity between predicted answer and real answer\n",
            "    is: 0.9256671985477677\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('new_result_with_pred.csv')"
      ],
      "metadata": {
        "id": "FNXacwId9K0x"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}