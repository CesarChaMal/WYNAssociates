{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Integrations of Vector Stores"
      ],
      "metadata": {
        "id": "ptW0LU-PRP7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Options"
      ],
      "metadata": {
        "id": "T2v08LI6UsbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximal Marginal Relevance (MMR)\n",
        "\n",
        "Maximal Marginal Relevance (MMR) is a method used in information retrieval and text summarization to identify and select diverse and relevant documents or parts of documents from a larger set of information. The goal of MMR is to balance two aspects:\n",
        "\n",
        "1. **Relevance**: How well the selected documents or snippets match the query or the main theme of the text.\n",
        "2. **Diversity**: How different each selected document or snippet is from others to avoid redundancy.\n",
        "\n",
        "#### Mathematical Formulation:\n",
        "\n",
        "The MMR score for a candidate item $D_i$ (a document or snippet) is calculated relative to a query $Q$ and a set of already-selected items $S$ using the following formula:\n",
        "\n",
        "$$\\text{MMR} = \\lambda \\cdot \\text{Relevance}(D_i, Q) - (1 - \\lambda) \\cdot \\max_{D_j \\in S} \\text{Similarity}(D_i, D_j)$$\n",
        "\n",
        "- $\\text{Relevance}(D_i, Q)$ measures how relevant document $D_i$ is to the query $Q$.\n",
        "- $\\text{Similarity}(D_i, D_j)$ measures how similar document $D_i$ is to another already-selected document $D_j$.\n",
        "- $\\lambda$ is a parameter that controls the trade-off between relevance and diversity. It ranges from 0 to 1, with higher values giving more weight to relevance and lower values emphasizing diversity.\n",
        "\n",
        "#### Intuition Behind MMR:\n",
        "\n",
        "- **When $\\lambda$ is high**: The algorithm focuses more on relevance, preferring items that are highly related to the query, even if they are similar to each other.\n",
        "- **When $\\lambda$ is low**: The algorithm emphasizes diversity, selecting items that are different from each other, even if they are less relevant to the query.\n",
        "\n",
        "This balancing act ensures that the final set of retrieved documents or snippets covers different aspects or viewpoints related to the query, providing a more comprehensive and diverse understanding of the topic, which is particularly valuable in contexts like search engine results, news aggregation, or literature reviews."
      ],
      "metadata": {
        "id": "WelTPf1mUxFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chroma\n",
        "\n",
        "[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.\n",
        "\n",
        "Chroma runs in various modes. See below for examples of each integrated with LangChain.\n",
        "\n",
        "- in-memory - in a python script or jupyter notebook\n",
        "- in-memory with persistance - in a script or notebook and save/load to disk\n",
        "- in a docker container - as a server running your local machine or in the cloud\n",
        "\n",
        "Like any other database, you can:\n",
        "\n",
        "- `.add`\n",
        "- `.get`\n",
        "- `.update`\n",
        "- `.upsert`\n",
        "- `.delete`\n",
        "- `.peek`\n",
        "and `.query` runs the similarity search.\n",
        "\n",
        "View full docs at [docs](https://docs.trychroma.com/reference/Collection). To access these methods directly, you can do `._collection.method()`\n",
        "\n"
      ],
      "metadata": {
        "id": "P3MQ1_L9Sf4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9vNdyUkPvJK"
      },
      "outputs": [],
      "source": [
        "pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "UuQAfjTqP9eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "MFzbg80PQUxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "drOaQcbnPvrg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example\n",
        "\n",
        "In this basic example, we take the most recent State of the Union Address, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it."
      ],
      "metadata": {
        "id": "ZLHtH2pGR6Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a fake txt file\n",
        "with open('some_file.txt', 'w') as f:\n",
        "  f.write(\n",
        "      \"\"\"\n",
        "        My mother's name is Jenny. \\n\n",
        "\n",
        "        My uncle's name is Tim. \\n\n",
        "\n",
        "        We live in Avatar City, Dumbo Street, United States of Dickens.\n",
        "      \"\"\"\n",
        "  )"
      ],
      "metadata": {
        "id": "z98S5cWjQliu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the document and split it into chunks\n",
        "loader = TextLoader(\"some_file.txt\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "tn7BqHlCPxj2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split it into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "WAE91llHP0oN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the open-source embedding function\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "vqhED-I2QRmw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load it into Chroma\n",
        "db = Chroma.from_documents(docs, embedding_function)"
      ],
      "metadata": {
        "id": "TqjSqTzcQSzj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query it\n",
        "query = \"What the mother's name?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdaMQ4-cQY9c",
        "outputId": "a99b841e-a1db-47b5-e67a-b89628baee3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My mother's name is Jenny. My uncle's name is Tim. We live in Avatar City, Dumbo Street, United States of Dickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query it\n",
        "query = \"Where do we live?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz91xgSHQcX2",
        "outputId": "9f991ddc-5bd0-4ab6-c768-09d5b055702c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My mother's name is Jenny. My uncle's name is Tim. We live in Avatar City, Dumbo Street, United States of Dickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query it\n",
        "query = \"What is 1+1?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z9L5fihQet9",
        "outputId": "6c59b119-a9a0-4aa3-8685-8fa93c348263"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My mother's name is Jenny. My uncle's name is Tim. We live in Avatar City, Dumbo Street, United States of Dickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faiss\n",
        "\n",
        "[Facebook AI Similarity Search (Faiss)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
        "\n",
        "[FAISS Doc](https://faiss.ai/)\n",
        "\n"
      ],
      "metadata": {
        "id": "42wlNonTTOY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows how to use functionality related to the FAISS vector database."
      ],
      "metadata": {
        "id": "wXoJp2FLTXh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For CUDA 7.5+ Supported GPU's.\n",
        "! pip install faiss-gpu\n",
        "# OR\n",
        "# pip install faiss-cpu # For CPU Installation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EIkWf4OR3bT",
        "outputId": "5f117ad1-c992-434e-b31b-eb1e1bbbfc79"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx6zyFD_T6KY",
        "outputId": "54ac9eb9-c097-42b2-93d1-48cb1bbc2067"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/220.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vChqHjymUAtN",
        "outputId": "95ac6308-e739-4084-e678-e0f0f0ade87b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "o9HAEIr5Ti2e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "weeI0fA2Tkf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example"
      ],
      "metadata": {
        "id": "jmfdqP7SUTlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "OG41_wLhTtr-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"some_file.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "7Zt7Qu_-TyIL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create FAISS database\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "sSmBSTAUT9_-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question\n",
        "query = \"What's the mother's name?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# answer\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Wm0O0QUFmF",
        "outputId": "634a8bf5-01b3-43b3-9939-5a5addf1de82"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My mother's name is Jenny. \n",
            "\n",
            "        \n",
            "        My uncle's name is Tim. \n",
            "\n",
            "        \n",
            "        We live in Avatar City, Dumbo Street, United States of Dickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more about langchain integrations of vector stores, please see [here](https://python.langchain.com/docs/integrations/vectorstores/chroma)."
      ],
      "metadata": {
        "id": "2UGmHVSdST3D"
      }
    }
  ]
}