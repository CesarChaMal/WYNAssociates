Papers introducing Foundational Computation Units and Neural Networks Training Machinery for Deep Learning:

1989, Convolution layer: https://lnkd.in/djSyctFV
1997, LSTM layer: https://lnkd.in/dxYAv8Er
2010, Glorot weight initialization technique: https://lnkd.in/dB5EffMw
2011, ReLU activation function: https://lnkd.in/dGbZm62D
2012, AdaDelta optimizer:https://lnkd.in/dYtE39Ny
2012, Random search for Hyper-Parameter Optimization: https://lnkd.in/dcEffyhB
2012, Feature dropout: https://lnkd.in/d5GFQmHZ
2012, Transfer learning: https://lnkd.in/dEwGCQXE
2013, Word2Vec Embedding: https://lnkd.in/dFEns8Mh
2013, Maxout network: https://lnkd.in/dvaD5YZk
2014, GRU layer: https://lnkd.in/dRjwNhaK
2014, Adam Optimizer: https://lnkd.in/dYEuEzRC
2014, Dropout layers: https://lnkd.in/dTxms3wj
2014, GloVe Embedding: https://lnkd.in/daA7ZFgf
2015, Batch normalization: https://lnkd.in/d9_msS9x
2015, PReLU: https://lnkd.in/di3ft6KS
2016, Layer normalization: https://lnkd.in/dXSwzTyq
2017, Self Attention layer: https://lnkd.in/dB5JytBJ
2017, Swish: Self-Gated https://lnkd.in/dEa5Uc3c
2018, BERT: https://lnkd.in/d72-YSvY