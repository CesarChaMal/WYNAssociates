# List of Important Papers

## Neural Net Fundamentals

Papers introducing Foundational Computation Units and Neural Networks Training Machinery for Deep Learning:

- 1989, Convolution layer: https://lnkd.in/djSyctFV
- 1997, LSTM layer: https://lnkd.in/dxYAv8Er
- 2010, Glorot weight initialization technique: https://lnkd.in/dB5EffMw
- 2011, ReLU activation function: https://lnkd.in/dGbZm62D
- 2012, AdaDelta optimizer:https://lnkd.in/dYtE39Ny
- 2012, Random search for Hyper-Parameter Optimization: https://lnkd.in/dcEffyhB
- 2012, Feature dropout: https://lnkd.in/d5GFQmHZ
- 2012, Transfer learning: https://lnkd.in/dEwGCQXE
- 2013, Word2Vec Embedding: https://lnkd.in/dFEns8Mh
- 2013, Maxout network: https://lnkd.in/dvaD5YZk
- 2014, GRU layer: https://lnkd.in/dRjwNhaK
- 2014, Adam Optimizer: https://lnkd.in/dYEuEzRC
- 2014, Dropout layers: https://lnkd.in/dTxms3wj
- 2014, GloVe Embedding: https://lnkd.in/daA7ZFgf
- 2015, Batch normalization: https://lnkd.in/d9_msS9x
- 2015, PReLU: https://lnkd.in/di3ft6KS
- 2016, Layer normalization: https://lnkd.in/dXSwzTyq
- 2017, Self Attention layer: https://lnkd.in/dB5JytBJ
- 2017, Swish: Self-Gated https://lnkd.in/dEa5Uc3c
- 2018, BERT: https://lnkd.in/d72-YSvY
- 2022, InstructGPT: https://arxiv.org/abs/2203.02155

## Image Segmentations

- 2018, Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang, UNet++: A Nested U-Net Architecture for Medical Image Segmentation
, https://arxiv.org/abs/1807.10165 (2018).
- 2019, Zhixuhao, Implementation of deep learning framework -- Unet, using Keras, https://github.com/zhixuhao/unet (2019).
- 2019, Clguo, Fully Dense UNet implementation in medical image segmentation, https://github.com/clguo/Dense_Unet_Keras (2019).
- 2019, Xu, X. Attention U-Net. https://www.kaggle.com/xxc025/attention-u-net (2019).
- 2020, Titu1994, Implementation of Squeeze and Excitation Networks in Keras, https://github.com/titu1994/keras-squeeze-excite-network (2020).
- 2019, S. Taghipour, UNet_ResNet34, https://www.kaggle.com/saeedtqp/unet-resnet34, (2019).
- 2019, lixiaolei1982, Keras Implementation of U-Net, R2U-Net, Attention U-Net, Attention R2U-Net, https://github.com/lixiaolei1982/Keras-Implementation-of-U-Net-R2U-Net-Attention-U-Net-Attention-R2U-Net.- (2019).
- 2019, danielenricocahall, Keras-UNet, https://github.com/danielenricocahall/Keras-UNet, (2019).
- 2020, Nahian Siddique, U-Net and its variants for medical image segmentation: theory and applications, https://arxiv.org/abs/2011.01118 (2020).
- 2021, longpollehn, A Keras implementation of UNet++, https://github.com/longmakesstuff/UnetPlusPlus, (2021).

## Explainable AI

- 2001, Friedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of statistics (2001): 1189-1232, https://www.jstor.org/stable/2699986 (2001).
- 2008, Friedman, Jerome H, and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals of Applied Statistics. JSTOR, 916–54. (2008), https://arxiv.org/abs/0811.1679 (2008). 
- 2013, Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013), https://arxiv.org/abs/1312.6034 (2013). 
- 2014, Štrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with feature contributions.” Knowledge and information systems 41.3 (2014): 647-665, https://link.springer.com/article/10.1007/s10115-013-0679-x (2014).
- 2016, Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of machine learning.” ICML Workshop on Human Interpretability in Machine Learning. https://arxiv.org/abs/1606.05386 (2016).
- 2016, Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016). https://proceedings.neurips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html (2016). 
- 2016, Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should I trust you?: Explaining the predictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016). https://arxiv.org/abs/1602.04938 (2016).
- 2017 (originally 1953), Shapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953): 307-317. https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html (2017).
- 2017, Doshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. http://arxiv.org/abs/1702.08608 (2017).
- 2017, Lundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems (2017). https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html (2017).
- 2018, Robnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning. Springer, Cham. 159-175. https://link.springer.com/chapter/10.1007/978-3-319-90403-0_9 (2018).
- 2018, Alvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018), http://128.84.21.203/abs/1806.08049 (2018). 
- 2018, Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. “Anchors: high-precision model-agnostic explanations”. AAAI Conference on Artificial Intelligence (AAAI), 2018. https://ojs.aaai.org/index.php/AAAI/article/view/11491 (2018). 
- 2018, Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously.” http://arxiv.org/abs/1801.01489 (2018).
- 2018, Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint arXiv:1804.01955 (2018), https://arxiv.org/abs/1804.01955 (2018). 
- 2018, Vitali Petsiuk, Abir Das, Kate Saenko, RISE: Randomized Input Sampling for Explanation of Black-box Models, https://arxiv.org/abs/1806.07421 (2018). 
- 2019, Sundararajan, Mukund, and Amir Najmi. “The many Shapley values for model explanation.” arXiv preprint arXiv:1908.08474 (2019), http://proceedings.mlr.press/v119/sundararajan20b.html (2019). 
- 2020, Apley, Daniel W., and Jingyu Zhu. “Visualizing the effects of predictor variables in black box supervised learning models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086. https://arxiv.org/abs/1612.08468 (2020). 
- 2020, Karimi, Amir-Hossein, Gilles Barthe, Borja Balle and Isabel Valera. “Model-agnostic counterfactual explanations for consequential decisions.” AISTATS (2020). https://proceedings.mlr.press/v108/karimi20a.html (2020). 
- 2020, Mothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. “Explaining machine learning classifiers through diverse counterfactual explanations.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. (2020), https://dl.acm.org/doi/abs/10.1145/3351095.3372850 (2020). 
- 2020, Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. “Feature relevance quantification in explainable AI: A causal problem.” International Conference on Artificial Intelligence and Statistics. PMLR (2020), https://arxiv.org/abs/1910.13413 (2020). 

